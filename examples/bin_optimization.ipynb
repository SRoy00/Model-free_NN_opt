{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a257f265",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e66b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and the custom optimizer\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "import dynamiqs as dq\n",
    "import tensorflow as tf\n",
    "\n",
    "import utilities as utl\n",
    "import ET_utilities as etutl\n",
    "from load_and_plot import plot_evolution\n",
    "from optimizers import HardwareAwareOptimizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "config.update(\"jax_enable_x64\", False)\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "dq.set_progress_meter(False)\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "# Set a global seed for reproducible results\n",
    "seed_value = 44\n",
    "tf.random.set_seed(seed_value)\n",
    "np.random.seed(seed_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cae8a6",
   "metadata": {},
   "source": [
    "# Binomial X gate:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d56e9a",
   "metadata": {},
   "source": [
    "## Pre-training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc52116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a sample pulse for pre-training...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C://Users//Fatem//SR_optctrl//Pulses//bin_X_gate.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading a sample pulse for pre-training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m directory = \u001b[33m'\u001b[39m\u001b[33mC://Users//Fatem//SR_optctrl//Pulses//\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbin_X_gate.npz\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m amps = data[\u001b[33m\"\u001b[39m\u001b[33marr_0\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      9\u001b[39m ideal_pulse = amps\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/optctrl/lib/python3.13/site-packages/numpy/lib/_npyio_impl.py:454\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    452\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     fid = stack.enter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    455\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C://Users//Fatem//SR_optctrl//Pulses//bin_X_gate.npz'"
     ]
    }
   ],
   "source": [
    "# Load a pulse obtained from gradient descent\n",
    "# It should be a numpy array of shape (4, n_pulse_points)\n",
    "print(\"Loading a sample pulse for pre-training...\")\n",
    "\n",
    "directory = '/Users/saswataroy/OptimalControl/pulses/'\n",
    "data = np.load(directory + 'bin_X_gate.npz')\n",
    "amps = data[\"arr_0\"]\n",
    "\n",
    "ideal_pulse = amps\n",
    "\n",
    "print(\"Sample pulse loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05cd00c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE SIMULATION AND PHYSICS PARAMETERS\n",
    "fourier_scale = 12\n",
    "# Instantiate the main optimizer class\n",
    "# Define the scaling factors for the output layer.\n",
    "# These values multiply the standard Glorot-initialized weights.\n",
    "# Format: (I_cavity, Q_cavity, I_qubit, Q_qubit)\n",
    "init_scales = (8, 8, 20, 20)\n",
    "\n",
    "# Simulation parameters\n",
    "osc_drive_type = 'linear'\n",
    "ncav = 15  # Hilbert space dimension for the cavity\n",
    "ntr = 2    # Hilbert space dimension for the transmon\n",
    "\n",
    "# Device parameters (in MHz, multiplied by 2*pi)\n",
    "alpha = -165 * (2 * np.pi)\n",
    "K = -0.0242 * (2 * np.pi)*0\n",
    "chi = -3.649 * (2 * np.pi)\n",
    "chi_prime = +0.039 * (2 * np.pi)\n",
    "\n",
    "# Pulse parameters\n",
    "T = 1  # Total pulse time in microseconds\n",
    "ntsave = 1001\n",
    "ntpulse = 1001\n",
    "max_amp = 10 * jnp.pi * 2 # Max amplitude for the drives\n",
    "max_freq = 50 # Max frequency component in MHz\n",
    "\n",
    "# SETUP HAMILTONIAN, STATES, AND OPERATORS ---\n",
    "\n",
    "# Operators\n",
    "a, adag = dq.destroy(ncav), dq.create(ncav)\n",
    "t, tdag = dq.destroy(ntr), dq.create(ntr)\n",
    "idcav, idtr = dq.eye(ncav), dq.eye(ntr)\n",
    "\n",
    "# Define Binomial codewords and errorwords\n",
    "L0 = (dq.basis(ncav, 0) + dq.basis(ncav, 4)) / jnp.sqrt(2)\n",
    "L1 = dq.basis(ncav, 2)\n",
    "\n",
    "plus_X = dq.unit((L0 + L1) / jnp.sqrt(2)) \n",
    "minus_X = dq.unit((L0 - L1) / jnp.sqrt(2))\n",
    "plus_Y = dq.unit((L0 + 1j*L1) / jnp.sqrt(2)) \n",
    "minus_Y = dq.unit((L0 - 1j*L1) / jnp.sqrt(2))\n",
    "\n",
    "# Error words:\n",
    "E0 = dq.basis(ncav, 3)\n",
    "E1 = dq.basis(ncav, 1)\n",
    "E_plus_X = dq.unit((E0 + E1) / jnp.sqrt(2))\n",
    "E_minus_X = dq.unit((E0 - E1) / jnp.sqrt(2))\n",
    "E_plus_Y = dq.unit((E0 + 1j*E1) / jnp.sqrt(2))\n",
    "E_minus_Y = dq.unit((E0 - 1j*E1) / jnp.sqrt(2))\n",
    "\n",
    "# Define initial and target states for the X-gate\n",
    "psi0 = [dq.tensor(L0, dq.basis(ntr, 0)), \n",
    "        dq.tensor(L1, dq.basis(ntr, 0)),\n",
    "        dq.tensor(plus_X, dq.basis(ntr, 0)),\n",
    "        dq.tensor(minus_X, dq.basis(ntr, 0)),\n",
    "        dq.tensor(plus_Y, dq.basis(ntr, 0)),\n",
    "        dq.tensor(minus_Y, dq.basis(ntr, 0)),\n",
    "        # dq.tensor(E0, dq.basis(ntr, 0)),\n",
    "        # dq.tensor(E1, dq.basis(ntr, 0)),\n",
    "        # dq.tensor(E_plus_X, dq.basis(ntr, 0)), \n",
    "        # dq.tensor(E_minus_X, dq.basis(ntr, 0)),\n",
    "        # dq.tensor(E_plus_Y, dq.basis(ntr, 0)), \n",
    "        # dq.tensor(E_minus_Y, dq.basis(ntr, 0))\n",
    "]\n",
    "\n",
    "exp_ops = [dq.tensor(dq.proj(L1), dq.proj(dq.basis(ntr, 0))), \n",
    "           dq.tensor(dq.proj(L0), dq.proj(dq.basis(ntr, 0))),\n",
    "           dq.tensor(dq.proj(plus_X), dq.proj(dq.basis(ntr, 0))), \n",
    "           dq.tensor(dq.proj(minus_X), dq.proj(dq.basis(ntr, 0))),\n",
    "           dq.tensor(dq.proj(minus_Y), dq.proj(dq.basis(ntr, 0))), \n",
    "           dq.tensor(dq.proj(plus_Y), dq.proj(dq.basis(ntr, 0))),\n",
    "        #    dq.tensor(dq.proj(E1), dq.proj(dq.basis(ntr, 0))), \n",
    "        #    dq.tensor(dq.proj(E0), dq.proj(dq.basis(ntr, 0))),\n",
    "        #    dq.tensor(dq.proj(E_plus_X), dq.proj(dq.basis(ntr, 0))), \n",
    "        #    dq.tensor(dq.proj(E_minus_X), dq.proj(dq.basis(ntr, 0))),\n",
    "        #    dq.tensor(dq.proj(E_minus_Y), dq.proj(dq.basis(ntr, 0))), \n",
    "        #    dq.tensor(dq.proj(E_plus_Y), dq.proj(dq.basis(ntr, 0)))\n",
    "]\n",
    "\n",
    "# Time arrays\n",
    "tsave = jnp.linspace(0.0, T, ntsave, dtype = jnp.float32)\n",
    "tpulse = jnp.linspace(0.0, T, ntpulse, dtype = jnp.float32)\n",
    "\n",
    "# Static Hamiltonian\n",
    "H0 = chi * dq.tensor(adag@a, tdag@t) + (K/2)*dq.tensor(adag@adag@a@a, idtr) + \\\n",
    "     (alpha/2)*dq.tensor(idcav, tdag@tdag@t@t) + (chi_prime/2)*dq.tensor(adag@adag@a@a, tdag@t)\n",
    "\n",
    "# Create the master parameter dictionary\n",
    "my_params = {'H0': H0, \n",
    "             'ncav': ncav, \n",
    "             'ntr': ntr, \n",
    "             'alpha': alpha, \n",
    "             'K': K, \n",
    "             'chi': chi,\n",
    "             'chi_prime': chi_prime, \n",
    "             'T': T, \n",
    "             'ntsave': ntsave, \n",
    "             'ntpulse': ntpulse,\n",
    "             'max_amp': max_amp, \n",
    "             'max_freq': max_freq, \n",
    "             'psi0': psi0, \n",
    "             'exp_ops': exp_ops, \n",
    "             'tsave': tsave, \n",
    "             'tpulse': tpulse, \n",
    "             'osc_drive': osc_drive_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5348e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Optimizer...\n",
      "Neural network model created.\n"
     ]
    }
   ],
   "source": [
    "optimizer = HardwareAwareOptimizer(my_params, \n",
    "                                   fourier_scale = fourier_scale,\n",
    "                                   output_scales = init_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa9607cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Pre-training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-training:   0%|          | 2151/1000000 [01:19<10:15:40, 27.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_89096\\3656812036.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Instantiate the main optimizer class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# optimizer = HardwareAwareOptimizer(my_params, fourier_scale=18)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Pre-train the network on the GRADIENT descent generated pulse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m optimizer.pre_train(\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mideal_pulse_arrays\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mideal_pulse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtime_array\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmy_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tpulse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m1000000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Fatem\\SR_optctrl\\NN_OptimalControl\\optimizers.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, ideal_pulse_arrays, time_array, epochs, learning_rate, print_every_x_steps, target_mse, plot_live)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[0mgenerated_pulse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_pulse_processing_tf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_pulse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;31m#  Use Mean Squared Error for the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerated_pulse\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtarget_pulse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1062\u001b[0m               \u001b[0moutput_gradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[0;32m   1064\u001b[0m                           \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutput_gradients\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1066\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     raise ValueError(\n\u001b[0;32m     65\u001b[0m         \u001b[1;34m\"Unknown value for unconnected_gradients: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0munconnected_gradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gradient_tape/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1651\u001b[0m   \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m   \u001b[0mzeros\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1653\u001b[0m   \u001b[0mgx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1654\u001b[0m   \u001b[0mgy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1655\u001b[1;33m   \u001b[0mgx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ReduceGradientArgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1656\u001b[0m   \u001b[0mgy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ReduceGradientArgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1657\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, gx, gy)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_ReduceGradientArgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;34m\"\"\"Reduces gradients of both arguments of a broadcasting binary op.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mgy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m     \u001b[0mbx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSmartBroadcastGradientArgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[0mgx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ReduceGradientArg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mgy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ReduceGradientArg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mgx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mx_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mx_axes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my_axes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;31m# NOTE: In graph mode, this is never exercised for statically known shapes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[0mx_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_gradient_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[0mx_must_reduce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0my_must_reduce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(s0, s1, name)\u001b[0m\n\u001b[0;32m    784\u001b[0m       \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_BroadcastGradientArgsOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 788\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    789\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m       return broadcast_gradient_args_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Instantiate the main optimizer class\n",
    "# optimizer = HardwareAwareOptimizer(my_params, fourier_scale=18)\n",
    "\n",
    "# Pre-train the network on the GRADIENT descent generated pulse\n",
    "optimizer.pre_train(\n",
    "    ideal_pulse_arrays=ideal_pulse,\n",
    "    time_array=my_params['tpulse'][:-1],\n",
    "    epochs= 1000000,\n",
    "    learning_rate=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea288f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying the pre-trained pulse...\n",
      "Generating pulse from the trained model...\n",
      "Mean Squared Error (NN vs Ideal): 14.315224\n"
     ]
    }
   ],
   "source": [
    "print(\"Verifying the pre-trained pulse...\")\n",
    "\n",
    "# Prepare the time vector input for the model\n",
    "time_array = my_params['tpulse'][:-1]\n",
    "time_input = tf.constant(time_array, dtype=tf.float64)\n",
    "time_input = tf.reshape(time_input, (1, len(time_array), 1))\n",
    "\n",
    "# Generate a pulse from the trained network\n",
    "# generated_pulse_tf = optimizer.model(time_input)\n",
    "# generated_pulse_np = generated_pulse_tf.numpy()\n",
    "\n",
    "generated_pulse_np = optimizer.generate_pulse()\n",
    "\n",
    "# Reconstruct the ideal pulse into the same complex format for comparison\n",
    "# I_c, Q_c, I_q, Q_q = [ideal_pulse[i] for i in range(4)]\n",
    "# drive_c_ideal = I_c + 1j * Q_c\"\n",
    "# drive_q_ideal = I_q + 1j * Q_q\n",
    "ideal_pulse_complex = amps\n",
    "\n",
    "# Numerically compare by calculating the Mean Squared Error\n",
    "mse = np.mean(np.abs(np.square(generated_pulse_np - ideal_pulse_complex)))\n",
    "print(f\"Mean Squared Error (NN vs Ideal): {mse:.6f}\")\n",
    "\n",
    "# Visually compare by plotting\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# Cavity Drive Plot\n",
    "axs[0].plot(time_array, np.real(ideal_pulse_complex[0, :]), 'b-', label='Ideal (Re)')\n",
    "axs[0].plot(time_array, np.imag(ideal_pulse_complex[0, :]), 'r-', label='Ideal (Im)')\n",
    "axs[0].plot(time_array, np.real(generated_pulse_np[0, :]), 'b--', label='NN (Re)')\n",
    "axs[0].plot(time_array, np.imag(generated_pulse_np[0, :]), 'r--', label='NN (Im)')\n",
    "axs[0].set_title('Cavity Drive Comparison')\n",
    "axs[0].set_ylabel('Amplitude')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Qubit Drive Plot\n",
    "axs[1].plot(time_array, np.real(ideal_pulse_complex[1, :]), 'b-', label='Ideal (Re)')\n",
    "axs[1].plot(time_array, np.imag(ideal_pulse_complex[1, :]), 'r-', label='Ideal (Im)')\n",
    "axs[1].plot(time_array, np.real(generated_pulse_np[1, :]), 'b--', label='NN (Re)')\n",
    "axs[1].plot(time_array, np.imag(generated_pulse_np[1, :]), 'r--', label='NN (Im)')\n",
    "axs[1].set_title('Qubit Drive Comparison')\n",
    "axs[1].set_xlabel('Time')\n",
    "axs[1].set_ylabel('Amplitude')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76b51dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved successfully to C://Users//Fatem//SR_optctrl//Pulses//pre_trained_bin_X_ET_2025-09-11_08-45-09.weights.h5\n",
      "C://Users//Fatem//SR_optctrl//Pulses//pre_trained_bin_X_ET_2025-09-11_08-45-09.weights.h5\n",
      "C://Users//Fatem//SR_optctrl//Pulses//pre_trained_bin_X_ET_2025-09-11_08-45-09.npz\n",
      "\n",
      "Pre-trained model is saved and ready for fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "directory = 'C://Users//Fatem//SR_optctrl//Pulses//'\n",
    "base_filename = 'pre_trained_bin_X_ET'\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "weights_filepath = directory + base_filename +'_'+ timestamp + '.weights.h5'\n",
    "params_filepath = directory + base_filename + '_'+ timestamp + '.npz'\n",
    "\n",
    "optimizer.save_model_weights(weights_filepath)\n",
    "np.savez(params_filepath, **my_params)\n",
    "print(weights_filepath)\n",
    "print(params_filepath)\n",
    "\n",
    "print(\"\\nPre-trained model is saved and ready for fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a5cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(weights_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ebde0d",
   "metadata": {},
   "source": [
    "## Offline Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d88c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Optimizer...\n",
      "Neural network model created.\n",
      "Model weights loaded successfully from C://Users//Fatem//SR_optctrl//Pulses//pre_trained_bin_X_ET_2025-09-11_08-45-09.weights.h5\n",
      "\n",
      " Model loaded, pre-training skipped\n"
     ]
    }
   ],
   "source": [
    "# Load the optimizer\n",
    "directory = 'C://Users//Fatem//SR_optctrl//Pulses//'\n",
    "\n",
    "optimizer = HardwareAwareOptimizer(my_params, \n",
    "                                   fourier_scale = fourier_scale,\n",
    "                                   output_scales = init_scales)\n",
    "# optimizer.load_model_weights(directory + 'pre_trained_bin_X_2025-09-02_10-07-09.weights.h5')\n",
    "# optimizer.load_model_weights(directory + 'pre_trained_bin_X_ET_2025-09-11_08-45-09.weights.h5')\n",
    "print('\\n Model loaded, pre-training skipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bc53093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_calculator_func(amps, params):\n",
    "\n",
    "    a, adag = dq.destroy(params['ncav']), dq.create(params['ncav'])\n",
    "    t, tdag = dq.destroy(params['ntr']), dq.create(params['ntr'])\n",
    "    idcav = dq.eye(params['ncav'])\n",
    "    idtr = dq.eye(params['ntr'])\n",
    "    \n",
    "    if params['osc_drive'] == 'linear':\n",
    "        osc_pow = 1\n",
    "    elif params['osc_drive']=='squeeze':\n",
    "        osc_pow = 2\n",
    "\n",
    "    # time-dependent Hamiltonian\n",
    "    # (sum of  piece-wise constant Hamiltonians and of the static Hamiltonian)\n",
    "    Hcr = dq.pwc(params['tpulse'], jnp.real(jnp.real(amps[0,:])), dq.tensor(dq.powm(a,osc_pow) + dq.powm(adag,osc_pow), idtr))\n",
    "    Hci = dq.pwc(params['tpulse'], jnp.real(jnp.imag(amps[0,:])), -1j * dq.tensor((dq.powm(a,osc_pow) - dq.powm(adag,osc_pow)), idtr))\n",
    "    Htr = dq.pwc(params['tpulse'], jnp.real(jnp.real(amps[1,:])), dq.tensor(idcav, t+tdag))\n",
    "    Hti = dq.pwc(params['tpulse'], jnp.real(jnp.imag(amps[1,:])), -1j * dq.tensor(idcav, (t - tdag)))\n",
    "    H = params['H0'] + Hcr + Hci + Htr + Hti\n",
    "\n",
    "    options = dq.Options(progress_meter = None)\n",
    "    solver = dq.method.Tsit5(max_steps = int(1e9))\n",
    "\n",
    "    evo_result = dq.sesolve(H, psi0, tsave, exp_ops = exp_ops, options = options, method = solver)\n",
    "    \n",
    "    avg_gate_fidelity_loss = utl.compute_fidelity_loss(evo_result)\n",
    "\n",
    "    loss = 0\n",
    "    cnt=0\n",
    "\n",
    "    loss += 1*avg_gate_fidelity_loss\n",
    "    cnt+=1\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9909aaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating pulse from the trained model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(0.95285344, dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_calculator_func(optimizer.generate_pulse(), my_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d66af3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Simulation Fine-tuning ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sim-tuning:   0%|          | 248/50000 [13:14<44:15:43,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target fidelity loss of 0.001 reached at epoch 248. Stopping fine-tuning.\n",
      "Simulation fine-tuning finished. Final Fidelity Loss: 0.000997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer.fine_tune_simulation(\n",
    "    loss_calculator_func=loss_calculator_func,\n",
    "    epochs=50000,\n",
    "    learning_rate=1e-3,\n",
    "    update_plot_every = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4a6319c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved successfully to /Users/saswataroy/OptimalControl/pulses/offline_trained_bin_X_2025-10-14_13-51-54.weights.h5\n",
      "'/Users/saswataroy/OptimalControl/pulses/offline_trained_bin_X_2025-10-14_13-51-54.weights.h5'\n",
      "'/Users/saswataroy/OptimalControl/pulses/offline_trained_bin_X_2025-10-14_13-51-54.npz'\n",
      "\n",
      "Trained model is saved and ready for hardware SPSA tuning\n"
     ]
    }
   ],
   "source": [
    "# Save trained model weights\n",
    "\n",
    "directory = '/Users/saswataroy/OptimalControl/pulses/'\n",
    "base_filename = 'offline_trained_bin_X'\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "weights_filepath = directory + base_filename +'_'+ timestamp + '.weights.h5'\n",
    "params_filepath = directory + base_filename + '_'+ timestamp + '.npz'\n",
    "\n",
    "optimizer.save_model_weights(weights_filepath)\n",
    "np.savez(params_filepath, **my_params)\n",
    "print(repr(weights_filepath))\n",
    "print(repr(params_filepath))\n",
    "\n",
    "print(\"\\nTrained model is saved and ready for hardware SPSA tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e715a3",
   "metadata": {},
   "source": [
    "### Robustness training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THiS Still has a wrong calculation of fidelity (not taking the diagonal elements)\n",
    "def robust_loss_calculator(amps, params):\n",
    "    \"\"\"\n",
    "    Calculates a weighted average of a central fidelity and a robust fidelity\n",
    "    over a distribution of Hamiltonian parameters in a single, efficient simulation.\n",
    "    \"\"\"\n",
    "    # Setup Operators and Parameter Variations ---\n",
    "    a, adag = dq.destroy(params['ncav']), dq.create(params['ncav'])\n",
    "    t, tdag = dq.destroy(params['ntr']), dq.create(params['ntr'])\n",
    "    idcav, idtr = dq.eye(params['ncav']), dq.eye(params['ntr'])\n",
    "\n",
    "    # Assumes the first entry in each list is the nominal (central) value - MHz without 2pi\n",
    "    chi_variations = jnp.array([-3.649, -3.649 - 0.04, -3.649 + 0.04], dtype = jnp.float32)*jnp.pi*2\n",
    "    # kerr_variations = jnp.array([-0.0242, -0.0242 - 0.005, -0.0242 + 0.005], dtype = jnp.float32)*jnp.pi*2\n",
    "    qb_dets_variations = jnp.array([0, -0.04, +0.04] , dtype = jnp.float32)*jnp.pi*2\n",
    "    \n",
    "    # Get other static parameters\n",
    "    # pi_32 = jnp.float32(jnp.pi)\n",
    "    alpha = jnp.float32(params['alpha'])\n",
    "    chi_prime = jnp.float32(params['chi_prime'])\n",
    "    K = jnp.float32(params['K'])\n",
    "    psi0 = params['psi0']\n",
    "    tsave = params['tsave']\n",
    "    exp_ops = params['exp_ops']\n",
    "\n",
    "    # Build the Time-Dependent Drive Hamiltonian (once) ---\n",
    "    if params['osc_drive'] == 'linear':\n",
    "        osc_pow = 1\n",
    "    elif params['osc_drive'] == 'squeeze':\n",
    "        osc_pow = 2\n",
    "\n",
    "    # Build the Batch of Hamiltonians ---\n",
    "    H0_batch_list = []\n",
    "    qb_drive_amps_list = []\n",
    "    for chi_mhz in chi_variations:\n",
    "        # for kerr_mhz in kerr_variations:\n",
    "        for qb_det in qb_dets_variations:\n",
    "\n",
    "            chi = chi_mhz \n",
    "            # K = kerr_mhz * (2 * jnp.pi)\n",
    "            H0_sample = (chi * dq.tensor(adag @ a, tdag @ t) +\n",
    "                        (K / 2) * dq.tensor(adag @ adag @ a @ a, idtr) +\n",
    "                        (alpha / 2) * dq.tensor(idcav, tdag @ tdag @ t @ t) +\n",
    "                        (chi_prime / 2) * dq.tensor(adag @ adag @ a @ a, tdag @ t))\n",
    "            \n",
    "            H0_batch_list.append(H0_sample)\n",
    "\n",
    "            qb_det_phi = qb_det * params['tpulse'][:-1]\n",
    "            det_qb_drive = amps[1,:] * jnp.exp(-1j*qb_det_phi)\n",
    "            qb_drive_amps_list.append(det_qb_drive)\n",
    "                \n",
    "\n",
    "    # Perform a batch simulation ---\n",
    "    H0_batch = dq.stack(H0_batch_list)\n",
    "    qb_drive_amps_batch = jnp.stack(qb_drive_amps_list)\n",
    "\n",
    "    osc_amps = amps[0,:]\n",
    "    osc_amps_batched = jnp.broadcast_to(osc_amps, qb_drive_amps_batch.shape)\n",
    "\n",
    "    Hcr = dq.pwc(params['tpulse'], jnp.real(jnp.real(osc_amps_batched)), dq.tensor(dq.powm(a,osc_pow) + dq.powm(adag,osc_pow), idtr))\n",
    "    Hci = dq.pwc(params['tpulse'], jnp.real(jnp.imag(osc_amps_batched)), -1j * dq.tensor((dq.powm(a,osc_pow) - dq.powm(adag,osc_pow)), idtr))\n",
    "    Htr = dq.pwc(params['tpulse'], jnp.real(jnp.real(qb_drive_amps_batch)), dq.tensor(idcav, t+tdag))\n",
    "    Hti = dq.pwc(params['tpulse'], jnp.real(jnp.imag(qb_drive_amps_batch)), -1j * dq.tensor(idcav, (t - tdag)))\n",
    "    \n",
    "    H_total_batch = H0_batch + Hcr + Hci + Htr + Hti\n",
    "\n",
    "    options = dq.Options(progress_meter=None)\n",
    "    solver = dq.method.Tsit5(max_steps=int(1e9))\n",
    "    batch_evo_result = dq.sesolve(\n",
    "        H_total_batch, psi0, tsave, exp_ops=exp_ops, options=options, method=solver\n",
    "    )\n",
    "\n",
    "    # The final expectation values for all simulations in the batch\n",
    "    all_expects_final = batch_evo_result.expects[..., -1]\n",
    "    \n",
    "    # The central fidelity is from the first simulation in the batch (index 0)\n",
    "    central_fidelity = jnp.mean(all_expects_final[0, :, :])\n",
    "    \n",
    "    # The robust fidelity is the average over all simulations in the batch\n",
    "    robust_fidelity = jnp.mean(all_expects_final)\n",
    "\n",
    "    # Combine and Return the Final Loss ---\n",
    "    robustness_weight = 0.0 \n",
    "    weighted_fidelity = (central_fidelity + robustness_weight * robust_fidelity) / (1 + robustness_weight)\n",
    "     \n",
    "    if central_fidelity > 0.999:\n",
    "        total_infidelity = 1.0 - central_fidelity\n",
    "    else:\n",
    "        total_infidelity = 1.0 - weighted_fidelity\n",
    "\n",
    "    return jnp.real(total_infidelity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2df4f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_loss_calculator_static(amps, params):\n",
    "    \"\"\"\n",
    "    Calculates robust loss by batching over only static Hamiltonian\n",
    "    parameters (chi and self-Kerr). The drive Hamiltonian is common to all simulations.\n",
    "    \"\"\"\n",
    "    # Operators and Parameter Variations\n",
    "    a, adag = dq.destroy(params['ncav']), dq.create(params['ncav'])\n",
    "    t, tdag = dq.destroy(params['ntr']), dq.create(params['ntr'])\n",
    "    idcav, idtr = dq.eye(params['ncav']), dq.eye(params['ntr'])\n",
    "\n",
    "    # Define variations for static parameters in MHz\n",
    "    chi_variations_mhz = jnp.array([-3.649, -3.649 - 0.05, -3.649 + 0.05], dtype=jnp.float32)*2*jnp.pi\n",
    "    kerr_variations_mhz = jnp.array([-0.0242, -0.0242 - 0.005, -0.0242 + 0.005], dtype=jnp.float32)*2*jnp.pi\n",
    "    \n",
    "    # Get constant parameters from the params dictionary\n",
    "    alpha = jnp.float32(params['alpha'])\n",
    "    chi_prime = jnp.float32(params['chi_prime'])\n",
    "    # psi0, tsave, exp_ops = params['psi0'], params['tsave'], params['exp_ops']\n",
    "\n",
    "    # Build the common Time-Dependent Drive Hamiltonian (once)\n",
    "    if params['osc_drive'] == 'linear':\n",
    "        osc_pow = 1\n",
    "    else:\n",
    "        osc_pow = 2\n",
    "    \n",
    "    Hcr = dq.pwc(params['tpulse'], jnp.real(amps[0,:]), dq.tensor(dq.powm(a,osc_pow) + dq.powm(adag,osc_pow), idtr))\n",
    "    Hci = dq.pwc(params['tpulse'], jnp.imag(amps[0,:]), -1j * dq.tensor((dq.powm(a,osc_pow) - dq.powm(adag,osc_pow)), idtr))\n",
    "    Htr = dq.pwc(params['tpulse'], jnp.real(amps[1,:]), dq.tensor(idcav, t+tdag))\n",
    "    Hti = dq.pwc(params['tpulse'], jnp.imag(amps[1,:]), -1j * dq.tensor(idcav, (t - tdag)))\n",
    "    H_drive = Hcr + Hci + Htr + Hti\n",
    "\n",
    "    #  Build a Batch of the static Hamiltonians \n",
    "    H0_batch_list = []\n",
    "    for chi_mhz in chi_variations_mhz:\n",
    "        for kerr_mhz in kerr_variations_mhz:\n",
    "            chi = chi_mhz \n",
    "            K = kerr_mhz \n",
    "            H0_sample = (chi * dq.tensor(adag @ a, tdag @ t) +\n",
    "                         (K / 2) * dq.tensor(adag @ adag @ a @ a, idtr) +\n",
    "                         (alpha / 2) * dq.tensor(idcav, tdag @ tdag @ t @ t) +\n",
    "                         (chi_prime / 2) * dq.tensor(adag @ adag @ a @ a, tdag @ t))\n",
    "            H0_batch_list.append(H0_sample)\n",
    "\n",
    "    # Stack the static Hamiltonians and add the common drive \n",
    "    H0_batch = dq.stack(H0_batch_list)\n",
    "    H_total_batch = H0_batch + H_drive\n",
    "\n",
    "    # Perform Simulation and Calculate Loss \n",
    "    options = dq.Options(progress_meter=None)\n",
    "    solver = dq.method.Tsit5(max_steps=int(1e9))\n",
    "    batch_evo_result = dq.sesolve(\n",
    "        H_total_batch, psi0, tsave, exp_ops=exp_ops, options=options, method=solver\n",
    "    )\n",
    "\n",
    "    all_expects_final = batch_evo_result.expects[:,:,:,-1].real\n",
    "    num_h, num_psi = (all_expects_final.shape)[0:2]\n",
    "    central_fidelity = sum(all_expects_final[0, i, i] for i in range(num_psi)) / num_psi\n",
    "    robust_fidelity = sum(sum(all_expects_final[:, i, i] for i in range(num_psi))) / (num_psi * num_h)\n",
    "    \n",
    "    robustness_weight=0.5\n",
    "    weighted_fidelity = (central_fidelity + robustness_weight * robust_fidelity) / (1 + robustness_weight)\n",
    "    \n",
    "    if central_fidelity>0.999:\n",
    "        total_infidelity = 1 - central_fidelity\n",
    "    else:\n",
    "        total_infidelity = 1.0 - weighted_fidelity\n",
    "    \n",
    "    return jnp.real(total_infidelity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f8b349",
   "metadata": {},
   "source": [
    "#### Time-dependent batched variations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74da4523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_robustness_tensors(params):\n",
    "    # Operators and Parameter Variations\n",
    "    a, adag = dq.destroy(params['ncav']), dq.create(params['ncav'])\n",
    "    t, tdag = dq.destroy(params['ntr']), dq.create(params['ntr'])\n",
    "    idcav, idtr = dq.eye(params['ncav']), dq.eye(params['ntr'])\n",
    "\n",
    "    # Define variations in MHz\n",
    "    chi_variations_mhz = jnp.array([-3.649, -3.649 - 0.005, -3.649 + 0.005], dtype=jnp.float32)\n",
    "    qb_dets_variations_mhz = jnp.array([0, -0.04, +0.04], dtype=jnp.float32)\n",
    "\n",
    "    # Get constant parameters\n",
    "    alpha = jnp.float32(params['alpha'])\n",
    "    chi_prime = jnp.float32(params['chi_prime'])\n",
    "    K = jnp.float32(params['K'])\n",
    "    pi_32 = jnp.float32(jnp.pi)\n",
    "\n",
    "    # Build a Batch of the static Hamiltonians and detuning phasors\n",
    "    H0_batch_list = []\n",
    "    phasor_batch_list = []\n",
    "    \n",
    "    for chi_mhz in chi_variations_mhz:\n",
    "        for qb_det_mhz in qb_dets_variations_mhz:\n",
    "            chi = chi_mhz * (2 * pi_32)\n",
    "            H0_sample = (chi * dq.tensor(adag @ a, tdag @ t) +\n",
    "                         (K / 2) * dq.tensor(adag @ adag @ a @ a, idtr) +\n",
    "                         (alpha / 2) * dq.tensor(idcav, tdag @ tdag @ t @ t) +\n",
    "                         (chi_prime / 2) * dq.tensor(adag @ adag @ a @ a, tdag @ t))\n",
    "            H0_batch_list.append(H0_sample)\n",
    "\n",
    "            qb_det = qb_det_mhz * (2 * pi_32)\n",
    "            qb_det_phase = qb_det * params['tpulse'][:-1]\n",
    "            phasor = jnp.exp(-1j * qb_det_phase)\n",
    "            phasor_batch_list.append(phasor)\n",
    "\n",
    "    # Return a dictionary of the pre-calculated, stacked tensors\n",
    "    return {\n",
    "        'H0_batch': dq.stack(H0_batch_list),\n",
    "        'phasor_batch': jnp.stack(phasor_batch_list)\n",
    "    }\n",
    "\n",
    "robustness_tensors = build_robustness_tensors(my_params)\n",
    "my_params.update(robustness_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d4fe28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Optimizer...\n",
      "Neural network model created.\n",
      "Initializing Optimizer...\n",
      "Neural network model created.\n",
      "Model weights loaded successfully from C://Users//Fatem//SR_optctrl//Pulses//pre_trained_bin_X_2025-09-02_10-07-09.weights.h5\n",
      "\n",
      " Model loaded, pre-training skipped\n"
     ]
    }
   ],
   "source": [
    "optimizer = HardwareAwareOptimizer(my_params)\n",
    "\n",
    "# Load the optimizer\n",
    "directory = 'C://Users//Fatem//SR_optctrl//Pulses//'\n",
    "\n",
    "optimizer = HardwareAwareOptimizer(my_params)\n",
    "optimizer.load_model_weights(directory + 'pre_trained_bin_X_2025-09-02_10-07-09.weights.h5')\n",
    "print('\\n Model loaded, pre-training skipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57589f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_loss_calculator(amps, params, robustness_weight=0.25):\n",
    "    # Operators and constant parameters\n",
    "    a, adag = dq.destroy(params['ncav']), dq.create(params['ncav'])\n",
    "    t, tdag = dq.destroy(params['ntr']), dq.create(params['ntr'])\n",
    "    idcav, idtr = dq.eye(params['ncav']), dq.eye(params['ntr'])\n",
    "    psi0, tsave, exp_ops = params['psi0'], params['tsave'], params['exp_ops']\n",
    "    \n",
    "    # Fetch pre-calculated tensors from the params dictionary\n",
    "    H0_batch = params['H0_batch']\n",
    "    phasor_batch = params['phasor_batch']\n",
    "    \n",
    "    # Build the time-dependent part of the Hamiltonian\n",
    "    if params['osc_drive'] == 'linear': osc_pow = 1\n",
    "    else: osc_pow = 2\n",
    "    \n",
    "    # Apply the pre-calculated phase to the qubit drive\n",
    "    det_qb_drive_batch = amps[1,:] * phasor_batch  # Broadcasting (B, N) = (N,) * (B, N)\n",
    "\n",
    "    # Broadcast the common cavity drive to match the batch shape\n",
    "    cavity_drive_amps = amps[0,:]\n",
    "    cavity_drive_amps_batch = jnp.broadcast_to(cavity_drive_amps, det_qb_drive_batch.shape)\n",
    "\n",
    "    # Construct the full batched Hamiltonian\n",
    "    Hcr = dq.pwc(params['tpulse'], jnp.real(cavity_drive_amps_batch), dq.tensor(dq.powm(a,osc_pow) + dq.powm(adag,osc_pow), idtr))\n",
    "    Hci = dq.pwc(params['tpulse'], jnp.imag(cavity_drive_amps_batch), -1j * dq.tensor((dq.powm(a,osc_pow) - dq.powm(adag,osc_pow)), idtr))\n",
    "    Htr = dq.pwc(params['tpulse'], jnp.real(det_qb_drive_batch), dq.tensor(idcav, t+tdag))\n",
    "    Hti = dq.pwc(params['tpulse'], jnp.imag(det_qb_drive_batch), -1j * dq.tensor(idcav, (t - tdag)))\n",
    "\n",
    "    H_total_batch = H0_batch + Hcr + Hci + Htr + Hti\n",
    "\n",
    "    # Perform Simulation and Calculate Loss\n",
    "    options = dq.Options(progress_meter=None)\n",
    "    solver = dq.method.Tsit5(max_steps=int(1e9))\n",
    "    batch_evo_result = dq.sesolve(\n",
    "        H_total_batch, psi0, tsave, exp_ops=exp_ops, options=options, method=solver\n",
    "    )\n",
    "\n",
    "    all_expects_final = batch_evo_result.expects[...,-1].real\n",
    "    num_psi = all_expects_final.shape[1]\n",
    "\n",
    "    # Vectorized fidelity calculation\n",
    "    traces = jnp.trace(all_expects_final, axis1=1, axis2=2)\n",
    "    central_fidelity = traces[0] / num_psi\n",
    "    robust_fidelity = jnp.mean(traces) / num_psi\n",
    "    \n",
    "    weighted_fidelity = (central_fidelity + robustness_weight * robust_fidelity) / (1 + robustness_weight)\n",
    "    \n",
    "    if central_fidelity > 0.999:\n",
    "        total_infidelity = 1 - central_fidelity\n",
    "    else:\n",
    "        total_infidelity = 1.0 - weighted_fidelity\n",
    "    \n",
    "    return jnp.real(total_infidelity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3918f2cf",
   "metadata": {},
   "source": [
    "#### static batched hamiltonian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d97a6902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Optimizer...\n",
      "Neural network model created.\n"
     ]
    }
   ],
   "source": [
    "def build_static_hamiltonian_batch(params):\n",
    "    # Operators and Parameter Variations\n",
    "    a, adag = dq.destroy(params['ncav']), dq.create(params['ncav'])\n",
    "    t, tdag = dq.destroy(params['ntr']), dq.create(params['ntr'])\n",
    "    idcav, idtr = dq.eye(params['ncav']), dq.eye(params['ntr'])\n",
    "\n",
    "    # Define variations for static parameters in MHz\n",
    "    chi_variations_mhz = jnp.array([-3.649, -3.649 - 0.03, -3.649 + 0.03], dtype=jnp.float32)\n",
    "    kerr_variations_mhz = jnp.array([-0.0242, -0.0242 - 0.005, -0.0242 + 0.005], dtype=jnp.float32)\n",
    "    \n",
    "    # Get constant parameters\n",
    "    alpha = jnp.float32(params['alpha'])\n",
    "    chi_prime = jnp.float32(params['chi_prime'])\n",
    "    pi_32 = jnp.float32(jnp.pi)\n",
    "    \n",
    "    # Build a Batch of the static Hamiltonians\n",
    "    H0_batch_list = []\n",
    "    for chi_mhz in chi_variations_mhz:\n",
    "        for kerr_mhz in kerr_variations_mhz:\n",
    "            chi = chi_mhz * (2 * pi_32)\n",
    "            K = kerr_mhz * (2 * pi_32)\n",
    "            H0_sample = (chi * dq.tensor(adag @ a, tdag @ t) +\n",
    "                         (K / 2) * dq.tensor(adag @ adag @ a @ a, idtr) +\n",
    "                         (alpha / 2) * dq.tensor(idcav, tdag @ tdag @ t @ t) +\n",
    "                         (chi_prime / 2) * dq.tensor(adag @ adag @ a @ a, tdag @ t))\n",
    "            H0_batch_list.append(H0_sample)\n",
    "            \n",
    "    return dq.stack(H0_batch_list)\n",
    "\n",
    "my_params['H0_batch'] = build_static_hamiltonian_batch(my_params)\n",
    "optimizer = HardwareAwareOptimizer(my_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "22490028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Optimizer...\n",
      "Neural network model created.\n",
      "Model weights loaded successfully from C://Users//Fatem//SR_optctrl//Pulses//pre_trained_bin_X_2025-09-02_10-07-09.weights.h5\n",
      "\n",
      " Model loaded, pre-training skipped\n"
     ]
    }
   ],
   "source": [
    "# Load the optimizer\n",
    "directory = 'C://Users//Fatem//SR_optctrl//Pulses//'\n",
    "\n",
    "optimizer = HardwareAwareOptimizer(my_params)\n",
    "optimizer.load_model_weights(directory + 'pre_trained_bin_X_2025-09-02_10-07-09.weights.h5')\n",
    "print('\\n Model loaded, pre-training skipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5e3445c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Optimizer...\n",
      "Neural network model created.\n",
      "Model weights loaded successfully from C://Users//Fatem//SR_optctrl//Pulses//simulation_trained_bin_X_robust_2025-09-04_18-22-06.weights.h5\n",
      "\n",
      " Model loaded, pre-training skipped\n"
     ]
    }
   ],
   "source": [
    "# Load the optimizer\n",
    "directory = 'C://Users//Fatem//SR_optctrl//Pulses//'\n",
    "\n",
    "optimizer = HardwareAwareOptimizer(my_params)\n",
    "optimizer.load_model_weights(directory + 'simulation_trained_bin_X_robust_2025-09-04_18-22-06.weights.h5')\n",
    "print('\\n Model loaded, pre-training skipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3d25cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def robust_loss_calculator_static(amps, params):\n",
    "    # Operators and constant parameters\n",
    "    a, adag = dq.destroy(params['ncav']), dq.create(params['ncav'])\n",
    "    t, tdag = dq.destroy(params['ntr']), dq.create(params['ntr'])\n",
    "    idcav, idtr = dq.eye(params['ncav']), dq.eye(params['ntr'])\n",
    "    psi0, tsave, exp_ops = params['psi0'], params['tsave'], params['exp_ops']\n",
    "    \n",
    "    # Build the common Time-Dependent Drive Hamiltonian\n",
    "    if params['osc_drive'] == 'linear':\n",
    "        osc_pow = 1\n",
    "    else:\n",
    "        osc_pow = 2\n",
    "    \n",
    "    Hcr = dq.pwc(params['tpulse'], jnp.real(amps[0,:]), dq.tensor(dq.powm(a,osc_pow) + dq.powm(adag,osc_pow), idtr))\n",
    "    Hci = dq.pwc(params['tpulse'], jnp.imag(amps[0,:]), -1j * dq.tensor((dq.powm(a,osc_pow) - dq.powm(adag,osc_pow)), idtr))\n",
    "    Htr = dq.pwc(params['tpulse'], jnp.real(amps[1,:]), dq.tensor(idcav, t+tdag))\n",
    "    Hti = dq.pwc(params['tpulse'], jnp.imag(amps[1,:]), -1j * dq.tensor(idcav, (t - tdag)))\n",
    "    H_drive = Hcr + Hci + Htr + Hti\n",
    "\n",
    "    # Fetch the pre-calculated static Hamiltonian batch and add the common drive\n",
    "    H0_batch = params['H0_batch']\n",
    "    H_total_batch = H0_batch + H_drive\n",
    "\n",
    "    # Perform Simulation and Calculate Loss\n",
    "    options = dq.Options(progress_meter=None)\n",
    "    solver = dq.method.Tsit5(max_steps=int(1e9))\n",
    "    batch_evo_result = dq.sesolve(\n",
    "        H_total_batch, psi0, tsave, exp_ops=exp_ops, options=options, method=solver\n",
    "    )\n",
    "\n",
    "    all_expects_final = batch_evo_result.expects[...,-1].real\n",
    "    num_psi = all_expects_final.shape[1]\n",
    "\n",
    "    # Vectorized fidelity calculation\n",
    "    # Takes the trace (sum of diagonals) for each matrix in the batch\n",
    "    traces = jnp.trace(all_expects_final, axis1=1, axis2=2)\n",
    "    \n",
    "    central_fidelity = traces[0] / num_psi\n",
    "    robust_fidelity = jnp.mean(traces) / num_psi\n",
    "    \n",
    "    robustness_weight=0.05\n",
    "    weighted_fidelity = (central_fidelity + robustness_weight * robust_fidelity) / (1 + robustness_weight)\n",
    "    \n",
    "    if central_fidelity > 0.999:\n",
    "        total_infidelity = 1 - central_fidelity\n",
    "    else:\n",
    "        total_infidelity = 1.0 - weighted_fidelity\n",
    "    \n",
    "    return jnp.real(total_infidelity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c518dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Simulation Fine-tuning ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sim-tuning:   0%|          | 28/60000 [09:09<198:37:20, 11.92s/it] "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "optimizer.fine_tune_simulation(\n",
    "    loss_calculator_func= robust_loss_calculator,\n",
    "    epochs=60000,\n",
    "    learning_rate=2.5e-4,\n",
    "    update_plot_every = 25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c5fcd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved successfully to C://Users//Fatem//SR_optctrl//Pulses//simulation_trained_bin_X_robust_ordinary_2025-09-06_22-45-17.weights.h5\n",
      "'C://Users//Fatem//SR_optctrl//Pulses//simulation_trained_bin_X_robust_ordinary_2025-09-06_22-45-17.weights.h5'\n",
      "'C://Users//Fatem//SR_optctrl//Pulses//simulation_trained_bin_X_robust_ordinary_2025-09-06_22-45-17.npz'\n",
      "\n",
      "Trained model is saved and ready for hardware SPSA tuning\n"
     ]
    }
   ],
   "source": [
    "# Save trained model weights\n",
    "\n",
    "directory = 'C://Users//Fatem//SR_optctrl//Pulses//'\n",
    "base_filename = 'simulation_trained_bin_X_robust_ordinary'\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "weights_filepath = directory + base_filename +'_'+ timestamp + '.weights.h5'\n",
    "params_filepath = directory + base_filename + '_'+ timestamp + '.npz'\n",
    "\n",
    "optimizer.save_model_weights(weights_filepath)\n",
    "np.savez(params_filepath, **my_params)\n",
    "print(repr(weights_filepath))\n",
    "print(repr(params_filepath))\n",
    "\n",
    "print(\"\\nTrained model is saved and ready for hardware SPSA tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ebbba",
   "metadata": {},
   "source": [
    "### ET training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b8dbc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def et_loss_calculator_func(amps, params):\n",
    "\n",
    "    a, adag = dq.destroy(params['ncav']), dq.create(params['ncav'])\n",
    "    t, tdag = dq.destroy(params['ntr']), dq.create(params['ntr'])\n",
    "    idcav = dq.eye(params['ncav'])\n",
    "    idtr = dq.eye(params['ntr'])\n",
    "    \n",
    "    if params['osc_drive'] == 'linear':\n",
    "        osc_pow = 1\n",
    "    elif params['osc_drive']=='squeeze':\n",
    "        osc_pow = 2\n",
    "\n",
    "    # time-dependent Hamiltonian\n",
    "    # (sum of  piece-wise constant Hamiltonians and of the static Hamiltonian)\n",
    "    Hcr = dq.pwc(params['tpulse'], jnp.real(jnp.real(amps[0,:])), dq.tensor(dq.powm(a,osc_pow) + dq.powm(adag,osc_pow), idtr))\n",
    "    Hci = dq.pwc(params['tpulse'], jnp.real(jnp.imag(amps[0,:])), -1j * dq.tensor((dq.powm(a,osc_pow) - dq.powm(adag,osc_pow)), idtr))\n",
    "    Htr = dq.pwc(params['tpulse'], jnp.real(jnp.real(amps[1,:])), dq.tensor(idcav, t+tdag))\n",
    "    Hti = dq.pwc(params['tpulse'], jnp.real(jnp.imag(amps[1,:])), -1j * dq.tensor(idcav, (t - tdag)))\n",
    "    H = params['H0'] + Hcr + Hci + Htr + Hti\n",
    "\n",
    "    options = dq.Options(progress_meter = None)\n",
    "    solver = dq.method.Tsit5(max_steps = int(1e9))\n",
    "\n",
    "    evo_result = dq.sesolve(H, psi0, tsave, exp_ops = exp_ops, options = options, method = solver)\n",
    "    \n",
    "    avg_gate_fidelity_loss = utl.compute_fidelity_loss(evo_result)\n",
    "\n",
    "    ET_fidelity_loss = etutl.compute_ET_fidelity_loss(evo_result, a, idtr)\n",
    "    \n",
    "    vel_var_loss = etutl.velocity_variance_loss(evo_result)\n",
    "\n",
    "    loss = avg_gate_fidelity_loss + 0.1*ET_fidelity_loss + 0.0*vel_var_loss\n",
    "\n",
    "    # print(avg_gate_fidelity_loss.value)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fb433aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Simulation Fine-tuning ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sim-tuning:   0%|          | 2/30000 [01:28<305:01:27, 36.61s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "optimizer.fine_tune_simulation(\n",
    "    loss_calculator_func=et_loss_calculator_func,\n",
    "    epochs=30000,\n",
    "    learning_rate=5e-4,\n",
    "    update_plot_every = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60225f6a",
   "metadata": {},
   "source": [
    "## SPSA testing in simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db1e60dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Optimizer...\n",
      "Neural network model created.\n",
      "Model weights loaded successfully from /Users/saswataroy/OptimalControl/pulses/offline_trained_bin_X_2025-10-14_13-51-54.weights.h5\n",
      "\n",
      "Model loaded, proceed to SPSA hardware tuning\n"
     ]
    }
   ],
   "source": [
    "# Load model weights\n",
    "\n",
    "directory = '/Users/saswataroy/OptimalControl/pulses/'\n",
    "optimizer = HardwareAwareOptimizer(my_params, \n",
    "                                   fourier_scale = fourier_scale,\n",
    "                                   output_scales = init_scales)\n",
    "optimizer.load_model_weights(directory + 'offline_trained_bin_X_2025-10-14_13-51-54.weights.h5')\n",
    "print('\\nModel loaded, proceed to SPSA hardware tuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f588115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating pulse from the trained model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(0.00103097, dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_calculator_func(optimizer.generate_pulse(), my_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "415c1703",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_params = my_params.copy()\n",
    "\n",
    "# Device parameters (in MHz, multiplied by 2*pi)\n",
    "alpha = -165 * (2 * np.pi)\n",
    "K = (-0.0242  + 0.005) * (2 * np.pi) * 0\n",
    "chi = (-3.649 + 0.01) * (2 * np.pi)\n",
    "chi_prime = (+0.039 + 0.0) * (2 * np.pi)\n",
    "\n",
    "# Detuning on qubit and cavity drives\n",
    "det_osc = 0.02*(2*np.pi)*1\n",
    "det_qb = -0.02*(2*np.pi)*1\n",
    "\n",
    "my_test_params['H0'] = chi * dq.tensor(adag@a, tdag@t) + (K/2)*dq.tensor(adag@adag@a@a, idtr) + \\\n",
    "        (alpha/2)*dq.tensor(idcav, tdag@tdag@t@t) + (chi_prime/2)*dq.tensor(adag@adag@a@a, tdag@t)\n",
    "\n",
    "def spsa_test_loss_calculator(amps, params):\n",
    "    \"\"\"\n",
    "    Test loss calculator for SPSA testing\n",
    "    \"\"\"\n",
    "    \n",
    "    a, adag = dq.destroy(params['ncav']), dq.create(params['ncav'])\n",
    "    t, tdag = dq.destroy(params['ntr']), dq.create(params['ntr'])\n",
    "    idcav = dq.eye(params['ncav'])\n",
    "    idtr = dq.eye(params['ntr'])\n",
    "    \n",
    "    if params['osc_drive'] == 'linear':\n",
    "        osc_pow = 1\n",
    "    elif params['osc_drive']=='squeeze':\n",
    "        osc_pow = 2\n",
    "\n",
    "    # Add noise to the drive\n",
    "    iq_gain_errors = 1.0 + np.random.uniform(-0.001, 0.001, size=(4,))\n",
    "    amps_real_noisy = np.real(amps) * np.array([[iq_gain_errors[0]], [iq_gain_errors[2]]])\n",
    "    amps_imag_noisy = np.imag(amps) * np.array([[iq_gain_errors[1]], [iq_gain_errors[3]]])\n",
    "\n",
    "    # noisy_amps = amps_real_noisy + 1j * amps_imag_noisy\n",
    "    noisy_amps = amps\n",
    "\n",
    "    # Add detuning to the drives\n",
    "    qb_det_phi = det_qb * params['tpulse'][:-1]\n",
    "    osc_det_phi = det_osc * params['tpulse'][:-1]\n",
    "    det_osc_drive = noisy_amps[0,:] * jnp.exp(-1j*osc_det_phi)\n",
    "    det_qb_drive = noisy_amps[1,:] * jnp.exp(-1j*qb_det_phi)\n",
    "\n",
    "    # time-dependent Hamiltonian\n",
    "    # (sum of  piece-wise constant Hamiltonians and of the static Hamiltonian)\n",
    "    Hcr = dq.pwc(params['tpulse'], jnp.real(jnp.real(det_osc_drive)), dq.tensor(dq.powm(a,osc_pow) + dq.powm(adag,osc_pow), idtr))\n",
    "    Hci = dq.pwc(params['tpulse'], jnp.real(jnp.imag(det_osc_drive)), -1j * dq.tensor((dq.powm(a,osc_pow) - dq.powm(adag,osc_pow)), idtr))\n",
    "    Htr = dq.pwc(params['tpulse'], jnp.real(jnp.real(det_qb_drive)), dq.tensor(idcav, t+tdag))\n",
    "    Hti = dq.pwc(params['tpulse'], jnp.real(jnp.imag(det_qb_drive)), -1j * dq.tensor(idcav, (t - tdag)))\n",
    "    H = params['H0'] + Hcr + Hci + Htr + Hti\n",
    "\n",
    "    options = dq.Options(progress_meter = None)\n",
    "    solver = dq.method.Tsit5(max_steps = int(1e9))\n",
    "\n",
    "    evo_result = dq.sesolve(H, psi0, tsave, exp_ops = exp_ops, options = options, method = solver)\n",
    "    \n",
    "    # add measurement noise to fidelity calculation\n",
    "    measurement_noise = 0.5*np.random.uniform(-0.01, 0.01)\n",
    "    avg_gate_fidelity_loss = np.clip(utl.compute_fidelity_loss(evo_result) + measurement_noise, 0, 1)\n",
    "\n",
    "    loss = 0\n",
    "    cnt=0\n",
    "\n",
    "    loss += 1*avg_gate_fidelity_loss\n",
    "    cnt+=1\n",
    "\n",
    "    return loss\n",
    "\n",
    "T1q = 70\n",
    "T2q = 30\n",
    "Tphiq = (1/T2q - 1/(2*T1q))**(-1)\n",
    "\n",
    "T1c = 200\n",
    "T2c = 280\n",
    "Tphic = (1/T2c - 1/(2*T1c))**(-1)\n",
    "\n",
    "c_ops = [\n",
    "        jnp.sqrt(1/T1q)*dq.tensor(idcav, t),\n",
    "        jnp.sqrt(1/Tphiq)*dq.tensor(idcav, tdag @ t),\n",
    "        jnp.sqrt(1/T1c)*dq.tensor(a, idtr), \n",
    "        jnp.sqrt(1/Tphic)*dq.tensor(adag @ a, idtr),\n",
    "        ]\n",
    "\n",
    "exp_state = [dq.tensor(dq.basis(ncav,2), dq.basis(ntr,0))]\n",
    "exp_code_dm_list = [psi @ psi.dag() for psi in exp_state]\n",
    "exp_code_dm = dq.stack(exp_code_dm_list)\n",
    "\n",
    "my_params['T1q'] = T1q\n",
    "my_params['T2q'] = T2q\n",
    "my_params['T1c'] = T1c\n",
    "my_params['T2c'] = T2c\n",
    "my_params['c_ops'] = c_ops\n",
    "my_params['exp_state'] = exp_state\n",
    "my_params['exp_code_dm'] = exp_code_dm\n",
    "\n",
    "def spsa_test_loss_calculator_mesolve(amps, params):\n",
    "    \"\"\"\n",
    "    Test loss calculator for SPSA testing\n",
    "    \"\"\"\n",
    "    \n",
    "    a, adag = dq.destroy(params['ncav']), dq.create(params['ncav'])\n",
    "    t, tdag = dq.destroy(params['ntr']), dq.create(params['ntr'])\n",
    "    idcav = dq.eye(params['ncav'])\n",
    "    idtr = dq.eye(params['ntr'])\n",
    "    \n",
    "    if params['osc_drive'] == 'linear':\n",
    "        osc_pow = 1\n",
    "    elif params['osc_drive']=='squeeze':\n",
    "        osc_pow = 2\n",
    "\n",
    "    # Add noise to the drive\n",
    "    # iq_gain_errors = 1.0 + np.random.uniform(-0.001, 0.001, size=(4,))\n",
    "    # amps_real_noisy = np.real(amps) * np.array([[iq_gain_errors[0]], [iq_gain_errors[2]]])\n",
    "    # amps_imag_noisy = np.imag(amps) * np.array([[iq_gain_errors[1]], [iq_gain_errors[3]]])\n",
    "\n",
    "    # noisy_amps = amps_real_noisy + 1j * amps_imag_noisy\n",
    "    noisy_amps = amps\n",
    "\n",
    "    # Add detuning to the drives\n",
    "    qb_det_phi = det_qb * params['tpulse'][:-1]\n",
    "    osc_det_phi = det_osc * params['tpulse'][:-1]\n",
    "    det_osc_drive = noisy_amps[0,:] * jnp.exp(-1j*osc_det_phi)\n",
    "    det_qb_drive = noisy_amps[1,:] * jnp.exp(-1j*qb_det_phi)\n",
    "\n",
    "    # time-dependent Hamiltonian\n",
    "    # (sum of  piece-wise constant Hamiltonians and of the static Hamiltonian)\n",
    "    Hcr = dq.pwc(params['tpulse'], jnp.real(jnp.real(det_osc_drive)), dq.tensor(dq.powm(a,osc_pow) + dq.powm(adag,osc_pow), idtr))\n",
    "    Hci = dq.pwc(params['tpulse'], jnp.real(jnp.imag(det_osc_drive)), -1j * dq.tensor((dq.powm(a,osc_pow) - dq.powm(adag,osc_pow)), idtr))\n",
    "    Htr = dq.pwc(params['tpulse'], jnp.real(jnp.real(det_qb_drive)), dq.tensor(idcav, t+tdag))\n",
    "    Hti = dq.pwc(params['tpulse'], jnp.real(jnp.imag(det_qb_drive)), -1j * dq.tensor(idcav, (t - tdag)))\n",
    "    H = params['H0'] + Hcr + Hci + Htr + Hti\n",
    "\n",
    "    options = dq.Options(progress_meter = None)\n",
    "    solver = dq.method.Tsit5(max_steps = int(1e9))\n",
    "\n",
    "    evo_result = dq.mesolve(H, rho0=psi0, tsave=tsave, jump_ops = c_ops, exp_ops = exp_ops, options = options, method = solver)\n",
    "    \n",
    "    # finals = evo_result.states[:,-1]\n",
    "    # fid = jnp.mean(jax.vmap(lambda r, t: jnp.abs(dq.trace(r@t)), in_axes=(0,0))(finals, exp_code_dm))\n",
    "\n",
    "   # add measurement noise to fidelity calculation\n",
    "    measurement_noise = 0*np.random.uniform(-0.01, 0.01)\n",
    "    avg_gate_fidelity_loss = np.clip(utl.compute_fidelity_loss(evo_result) + measurement_noise, 0, 1)\n",
    "\n",
    "    loss = 0\n",
    "    cnt=0\n",
    "\n",
    "    loss += 1*avg_gate_fidelity_loss\n",
    "    cnt+=1\n",
    "\n",
    "    return loss\n",
    "\n",
    "def loss_calculator_func(amps, params):\n",
    "\n",
    "    a, adag = dq.destroy(params['ncav']), dq.create(params['ncav'])\n",
    "    t, tdag = dq.destroy(params['ntr']), dq.create(params['ntr'])\n",
    "    idcav = dq.eye(params['ncav'])\n",
    "    idtr = dq.eye(params['ntr'])\n",
    "    \n",
    "    if params['osc_drive'] == 'linear':\n",
    "        osc_pow = 1\n",
    "    elif params['osc_drive']=='squeeze':\n",
    "        osc_pow = 2\n",
    "\n",
    "    # time-dependent Hamiltonian\n",
    "    Hcr = dq.pwc(params['tpulse'], jnp.real(jnp.real(amps[0,:])), dq.tensor(dq.powm(a,osc_pow) + dq.powm(adag,osc_pow), idtr))\n",
    "    Hci = dq.pwc(params['tpulse'], jnp.real(jnp.imag(amps[0,:])), -1j * dq.tensor((dq.powm(a,osc_pow) - dq.powm(adag,osc_pow)), idtr))\n",
    "    Htr = dq.pwc(params['tpulse'], jnp.real(jnp.real(amps[1,:])), dq.tensor(idcav, t+tdag))\n",
    "    Hti = dq.pwc(params['tpulse'], jnp.real(jnp.imag(amps[1,:])), -1j * dq.tensor(idcav, (t - tdag)))\n",
    "    H = params['H0'] + Hcr + Hci + Htr + Hti\n",
    "\n",
    "    options = dq.Options(progress_meter = None)\n",
    "    solver = dq.method.Tsit5(max_steps = int(1e9))\n",
    "\n",
    "    evo_result = dq.sesolve(H, psi0, tsave, exp_ops = exp_ops, options = options, method = solver)\n",
    "    \n",
    "    avg_gate_fidelity_loss = utl.compute_fidelity_loss(evo_result)\n",
    "\n",
    "    loss = 0\n",
    "    cnt=0\n",
    "\n",
    "    loss += 1*avg_gate_fidelity_loss\n",
    "    cnt+=1\n",
    "\n",
    "    return loss\n",
    "\n",
    "def loss_calculator_func_mesolve(amps, params):\n",
    "\n",
    "    a, adag = dq.destroy(params['ncav']), dq.create(params['ncav'])\n",
    "    t, tdag = dq.destroy(params['ntr']), dq.create(params['ntr'])\n",
    "    idcav = dq.eye(params['ncav'])\n",
    "    idtr = dq.eye(params['ntr'])\n",
    "    \n",
    "    if params['osc_drive'] == 'linear':\n",
    "        osc_pow = 1\n",
    "    elif params['osc_drive']=='squeeze':\n",
    "        osc_pow = 2\n",
    "\n",
    "    # time-dependent Hamiltonian\n",
    "    Hcr = dq.pwc(params['tpulse'], jnp.real(jnp.real(amps[0,:])), dq.tensor(dq.powm(a,osc_pow) + dq.powm(adag,osc_pow), idtr))\n",
    "    Hci = dq.pwc(params['tpulse'], jnp.real(jnp.imag(amps[0,:])), -1j * dq.tensor((dq.powm(a,osc_pow) - dq.powm(adag,osc_pow)), idtr))\n",
    "    Htr = dq.pwc(params['tpulse'], jnp.real(jnp.real(amps[1,:])), dq.tensor(idcav, t+tdag))\n",
    "    Hti = dq.pwc(params['tpulse'], jnp.real(jnp.imag(amps[1,:])), -1j * dq.tensor(idcav, (t - tdag)))\n",
    "    H = params['H0'] + Hcr + Hci + Htr + Hti\n",
    "\n",
    "    options = dq.Options(progress_meter = None)\n",
    "    solver = dq.method.Tsit5(max_steps = int(1e9))\n",
    "\n",
    "    evo_result = dq.mesolve(H, rho0=psi0, tsave=tsave, jump_ops = c_ops, exp_ops = exp_ops, options = options, method = solver)\n",
    "    \n",
    "    avg_gate_fidelity_loss = utl.compute_fidelity_loss(evo_result)\n",
    "\n",
    "    loss = 0\n",
    "    cnt=0\n",
    "\n",
    "    loss += 1*avg_gate_fidelity_loss\n",
    "    cnt+=1\n",
    "\n",
    "    return loss\n",
    "\n",
    "def fourier_transform_rows(amps_time: np.ndarray, dt: float = 1e-9):\n",
    "    \"\"\"\n",
    "    Compute the FFT of each row of a 2N time-domain array.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    amps_time : np.ndarray, shape (2, N)\n",
    "        Time-domain complex amplitudes (row 0 = oscillator, row 1 = qubit).\n",
    "    dt : float\n",
    "        Time step in seconds between samples (default 1e-9 s).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    freqs : np.ndarray, shape (N,)\n",
    "        Frequency bins in Hz, as returned by np.fft.fftfreq.\n",
    "    eps_freq : np.ndarray, shape (2, N)\n",
    "        Complex amplitudes in the frequency domain.\n",
    "    \"\"\"\n",
    "    N = amps_time.shape[1]\n",
    "    freqs = np.fft.fftfreq(N, d=dt)\n",
    "    eps_freq = np.fft.fft(amps_time, axis=1)\n",
    "    return freqs, eps_freq\n",
    "\n",
    "def inverse_fourier_transform_rows(eps_mod: np.ndarray):\n",
    "    \"\"\"\n",
    "    Inverse FFT to go back to a 2N time-domain waveform.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eps_mod : np.ndarray, shape (2, N)\n",
    "        Frequency-domain amplitudes (modified).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    amps_time_mod : np.ndarray, shape (2, N)\n",
    "        Complex time-domain amplitudes, at the original 1 ns sampling.\n",
    "    \"\"\"\n",
    "    amps_time_mod = np.fft.ifft(eps_mod, axis=1)\n",
    "    return amps_time_mod\n",
    "\n",
    "def dispersion_tuning(amps: np.ndarray,\n",
    "                        b_osc: float, b_qb: float,\n",
    "                        tau_osc: float, tau_qb: float,\n",
    "                        dt: float = 1e-9) -> np.ndarray:\n",
    "    freqs, eps_freq = fourier_transform_rows(amps, dt=dt)\n",
    "\n",
    "     = 2 * np.pi * freqs    \n",
    "    f_osc = 1 + b_osc *  * np.exp(1j *  * tau_osc)\n",
    "    f_qb  = 1 + b_qb  *  * np.exp(1j *  * tau_qb)\n",
    "    \n",
    "    factors = np.vstack([f_osc, f_qb])  \n",
    "\n",
    "    eps_mod = eps_freq * factors       \n",
    "\n",
    "    amps_mod = inverse_fourier_transform_rows(eps_mod)\n",
    "    return amps_mod  \n",
    "\n",
    "\n",
    "def spsa_test_loss_calculator_dispersion_mesolve(amps, params):\n",
    "    \"\"\"\n",
    "    Test loss calculator for SPSA testing\n",
    "    \"\"\"\n",
    "    \n",
    "    a, adag = dq.destroy(params['ncav']), dq.create(params['ncav'])\n",
    "    t, tdag = dq.destroy(params['ntr']), dq.create(params['ntr'])\n",
    "    idcav = dq.eye(params['ncav'])\n",
    "    idtr = dq.eye(params['ntr'])\n",
    "    \n",
    "    if params['osc_drive'] == 'linear':\n",
    "        osc_pow = 1\n",
    "    elif params['osc_drive']=='squeeze':\n",
    "        osc_pow = 2\n",
    "\n",
    "    # Add noise to the drive\n",
    "    # iq_gain_errors = 1.0 + np.random.uniform(-0.001, 0.001, size=(4,))\n",
    "    # amps_real_noisy = np.real(amps) * np.array([[iq_gain_errors[0]], [iq_gain_errors[2]]])\n",
    "    # amps_imag_noisy = np.imag(amps) * np.array([[iq_gain_errors[1]], [iq_gain_errors[3]]])\n",
    "\n",
    "    # noisy_amps = amps_real_noisy + 1j * amps_imag_noisy\n",
    "\n",
    "    amps = dispersion_tuning(amps, 0, 0.001*1e-6, 0, 0)\n",
    "    noisy_amps = amps\n",
    "\n",
    "    # Add detuning to the drives\n",
    "    qb_det_phi = det_qb * params['tpulse'][:-1]\n",
    "    osc_det_phi = det_osc * params['tpulse'][:-1]\n",
    "    det_osc_drive = noisy_amps[0,:] * jnp.exp(-1j*osc_det_phi)\n",
    "    det_qb_drive = noisy_amps[1,:] * jnp.exp(-1j*qb_det_phi)\n",
    "\n",
    "    # time-dependent Hamiltonian\n",
    "    # (sum of  piece-wise constant Hamiltonians and of the static Hamiltonian)\n",
    "    Hcr = dq.pwc(params['tpulse'], jnp.real(jnp.real(det_osc_drive)), dq.tensor(dq.powm(a,osc_pow) + dq.powm(adag,osc_pow), idtr))\n",
    "    Hci = dq.pwc(params['tpulse'], jnp.real(jnp.imag(det_osc_drive)), -1j * dq.tensor((dq.powm(a,osc_pow) - dq.powm(adag,osc_pow)), idtr))\n",
    "    Htr = dq.pwc(params['tpulse'], jnp.real(jnp.real(det_qb_drive)), dq.tensor(idcav, t+tdag))\n",
    "    Hti = dq.pwc(params['tpulse'], jnp.real(jnp.imag(det_qb_drive)), -1j * dq.tensor(idcav, (t - tdag)))\n",
    "    H = params['H0'] + Hcr + Hci + Htr + Hti\n",
    "\n",
    "    options = dq.Options(progress_meter = None)\n",
    "    solver = dq.method.Tsit5(max_steps = int(1e9))\n",
    "\n",
    "    evo_result = dq.mesolve(H, rho0=psi0, tsave=tsave, jump_ops = c_ops, exp_ops = exp_ops, options = options, method = solver)\n",
    "    \n",
    "    # finals = evo_result.states[:,-1]\n",
    "    # fid = jnp.mean(jax.vmap(lambda r, t: jnp.abs(dq.trace(r@t)), in_axes=(0,0))(finals, exp_code_dm))\n",
    "\n",
    "   # add measurement noise to fidelity calculation\n",
    "    measurement_noise = 0*np.random.uniform(-0.01, 0.01)\n",
    "    avg_gate_fidelity_loss = np.clip(utl.compute_fidelity_loss(evo_result) + measurement_noise, 0, 1)\n",
    "\n",
    "    loss = 0\n",
    "    cnt=0\n",
    "\n",
    "    loss += 1*avg_gate_fidelity_loss\n",
    "    cnt+=1\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6053bc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating pulse from the trained model...\n",
      "Generated pulse shape: (2, 1000)\n",
      "\n",
      "fidelity without parameter deviations and losses: 0.99228185\n",
      "fidelity with losses but no parameter deviation: 0.96777976\n",
      "\n",
      "Fidelity in experiment without losses before SPSA optimization: 0.974184\n",
      "Fidelity in experiment with losses before SPSA optimization: 0.95233315\n"
     ]
    }
   ],
   "source": [
    "gen_pulse = optimizer.generate_pulse()\n",
    "print(\"Generated pulse shape:\", gen_pulse.shape)\n",
    "\n",
    "# This gives the target or best possible scenario after SPSA optimization\n",
    "print('\\nfidelity without parameter deviations and losses:', 1 - loss_calculator_func(gen_pulse, my_params))\n",
    "print('fidelity with losses but no parameter deviation:', 1 - loss_calculator_func_mesolve(gen_pulse, my_params))\n",
    "\n",
    "# This is the starting point for SPSA: shows how much deviation we have\n",
    "print('\\nFidelity in experiment without losses before SPSA optimization:', 1 - spsa_test_loss_calculator(gen_pulse, my_test_params))\n",
    "print('Fidelity in experiment with losses before SPSA optimization:', 1- spsa_test_loss_calculator_mesolve(gen_pulse, my_test_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99b1c5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Hardware Fine-tuning (SPSA on ALL weights of layer: final_hidden_layer) ---\n",
      "Targeting 260 parameters in layer 'final_hidden_layer'.\n",
      "Initial loss before SPSA: 0.047667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SPSA Tuning (final_hidden_layer): 100%|| 1000/1000 [14:56<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization finished. Restoring best parameters found with loss: 0.029824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer.fine_tune_hardware_spsa_all(\n",
    "    test_loss_calculator=spsa_test_loss_calculator_mesolve,\n",
    "    test_params=my_test_params,\n",
    "    layer_name='final_hidden_layer', # Specify the layer to tune\n",
    "    spsa_steps=1000,                 # NOTE: May need more steps\n",
    "    a=0.01,                        # NOTE: May need a smaller step size\n",
    "    c=0.01,\n",
    "    update_plot_every=50,\n",
    "    target_fidelity_loss=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18c1afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.fine_tune_hardware_spsa(\n",
    "#     test_loss_calculator=spsa_test_loss_calculator,\n",
    "#     test_params=my_test_params,\n",
    "#     spsa_steps=5000,\n",
    "#     a=0.01,  # SPSA step size\n",
    "#     c=0.01,  # SPSA perturbation size\n",
    "#     update_plot_every=10,\n",
    "#     target_fidelity_loss=1e-3\n",
    "# )\n",
    "\n",
    "# print(\"\\nSPSA simulation test complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04e12730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.fine_tune_hardware_spsa_all(\n",
    "#     test_loss_calculator=spsa_test_loss_calculator,\n",
    "#     test_params=my_test_params,\n",
    "#     layer_name='final_hidden_layer', # Specify the layer to tune\n",
    "#     spsa_steps=1500,                 # NOTE: May need more steps\n",
    "#     a=0.01,                        # NOTE: May need a smaller step size\n",
    "#     c=0.01,\n",
    "#     update_plot_every=10,\n",
    "#     target_fidelity_loss=1e-3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea083884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating pulse from the trained model...\n",
      "Generated pulse shape: (2, 1000)\n",
      "Fidelity in experiment after SPSA optimization: 0.015979666\n"
     ]
    }
   ],
   "source": [
    "spsa_pulse = optimizer.generate_pulse()\n",
    "print(\"Generated pulse shape:\", spsa_pulse.shape)\n",
    "print('Fidelity in experiment after SPSA optimization:', spsa_test_loss_calculator(spsa_pulse, my_test_params))\n",
    "\n",
    "# Visually compare by plotting\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "time_array = my_params['tpulse'][:-1]\n",
    "\n",
    "# Cavity Drive Plot\n",
    "axs[0].plot(time_array, np.real(gen_pulse[0, :]), 'b-', label='sim (Re)')\n",
    "axs[0].plot(time_array, np.imag(gen_pulse[0, :]), 'r-', label='sim (Im)')\n",
    "axs[0].plot(time_array, np.real(spsa_pulse[0, :]), 'b--', label='SPSA (Re)')\n",
    "axs[0].plot(time_array, np.imag(spsa_pulse[0, :]), 'r--', label='SPSA (Im)')\n",
    "axs[0].set_title('Cavity Drive Comparison')\n",
    "axs[0].set_ylabel('Amplitude')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Qubit Drive Plot\n",
    "axs[1].plot(time_array, np.real(gen_pulse[1, :]), 'b-', label='sim (Re)')\n",
    "axs[1].plot(time_array, np.imag(gen_pulse[1, :]), 'r-', label='sim (Im)')\n",
    "axs[1].plot(time_array, np.real(spsa_pulse[1, :]), 'b--', label='SPSA (Re)')\n",
    "axs[1].plot(time_array, np.imag(spsa_pulse[1, :]), 'r--', label='SPSA (Im)')\n",
    "axs[1].set_title('Qubit Drive Comparison')\n",
    "axs[1].set_xlabel('Time')\n",
    "axs[1].set_ylabel('Amplitude')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc4d019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model weights\n",
    "\n",
    "directory = '/Users/saswataroy/OptimalControl/pulses/'\n",
    "base_filename = 'spsa_trained_bin_X_gate'\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "weights_filepath = directory + base_filename +'_'+ timestamp + '.weights.h5'\n",
    "params_filepath = directory + base_filename + '_'+ timestamp + '.npz'\n",
    "\n",
    "optimizer.save_model_weights(weights_filepath)\n",
    "np.savez(params_filepath, **my_test_params)\n",
    "print(repr(weights_filepath))\n",
    "print(repr(params_filepath))\n",
    "\n",
    "print(\"\\n Model trained on hardware is saved, ready for experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818317ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights\n",
    "\n",
    "directory = '/Users/saswataroy/OptimalControl/pulses/'\n",
    "optimizer = HardwareAwareOptimizer(my_test_params)\n",
    "optimizer.load_model_weights(directory + 'SPSA_tuned_model.weights.h5')\n",
    "print('\\n SPSA-tuned model loaded, proceed to experiments')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f4a16d",
   "metadata": {},
   "source": [
    "## Analysis: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34819f1b",
   "metadata": {},
   "source": [
    "### Evolution and pulse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad325e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Optimizer...\n",
      "Neural network model created.\n",
      "Model weights loaded successfully from C://Users//Fatem//SR_optctrl//Pulses//simulation_trained_bin_X_2025-09-02_13-20-53.weights.h5\n",
      "Model weights loaded successfully from C://Users//Fatem//SR_optctrl//Pulses//simulation_trained_bin_X_robust_2025-09-03_09-22-06.weights.h5\n",
      "Generating pulse from the trained model...\n",
      "Generating pulse from the trained model...\n"
     ]
    }
   ],
   "source": [
    "optimizer2 = HardwareAwareOptimizer(my_params)\n",
    "optimizer.load_model_weights(directory + 'simulation_trained_bin_X_2025-09-02_13-20-53.weights.h5')\n",
    "optimizer2.load_model_weights(directory + 'simulation_trained_bin_X_robust_2025-09-03_09-22-06.weights.h5')\n",
    "\n",
    "gen_pulse = optimizer.generate_pulse()\n",
    "rob_pulse = optimizer2.generate_pulse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef79134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010128518\n",
      "0.9749402\n"
     ]
    }
   ],
   "source": [
    "print(loss_calculator_func(gen_pulse, my_params))\n",
    "print(loss_calculator_func(rob_pulse, my_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cfa2f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 96\u001b[0m\n\u001b[0;32m     92\u001b[0m             prob_qb_sim[j,i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreal(dq\u001b[38;5;241m.\u001b[39mptrace(evo_result\u001b[38;5;241m.\u001b[39mstates[\u001b[38;5;241m0\u001b[39m, i], dims \u001b[38;5;241m=\u001b[39m (ncav, ntr), keep \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m,))\u001b[38;5;241m.\u001b[39mto_jax()[j,j])\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtsave\u001b[39m\u001b[38;5;124m'\u001b[39m], prob_sim, prob_qb_sim\n\u001b[1;32m---> 96\u001b[0m tsave, prob_sim_gen, prob_qb_sim_gen \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_evolution_mesolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_pulse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_test_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m _, prob_sim_spsa, prob_qb_sim_spsa \u001b[38;5;241m=\u001b[39m compute_evolution_mesolve(spsa_pulse, my_test_params)\n",
      "Cell \u001b[1;32mIn[10], line 81\u001b[0m, in \u001b[0;36mcompute_evolution_mesolve\u001b[1;34m(amps, params)\u001b[0m\n\u001b[0;32m     78\u001b[0m options \u001b[38;5;241m=\u001b[39m dq\u001b[38;5;241m.\u001b[39mOptions(progress_meter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     79\u001b[0m solver \u001b[38;5;241m=\u001b[39m dq\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mTsit5(max_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1e9\u001b[39m))\n\u001b[1;32m---> 81\u001b[0m evo_result \u001b[38;5;241m=\u001b[39m \u001b[43mdq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmesolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpsi0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtsave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtsave\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjump_ops\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_ops\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexp_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m prob_sim \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mncav\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mlen\u001b[39m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtsave\u001b[39m\u001b[38;5;124m'\u001b[39m])))\n\u001b[0;32m     84\u001b[0m prob_qb_sim \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mntr\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mlen\u001b[39m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtsave\u001b[39m\u001b[38;5;124m'\u001b[39m])))\n",
      "File \u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\dynamiqs\\integrators\\apis\\mesolve.py:237\u001b[0m, in \u001b[0;36mmesolve\u001b[1;34m(H, jump_ops, rho0, tsave, exp_ops, method, gradient, options)\u001b[0m\n\u001b[0;32m    233\u001b[0m rho0 \u001b[38;5;241m=\u001b[39m check_hermitian(rho0, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrho0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# we implement the jitted vectorization in another function to pre-convert QuTiP\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# objects (which are not JIT-compatible) to qarrays\u001b[39;00m\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_vectorized_mesolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtsave\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\dynamiqs\\integrators\\_utils.py:56\u001b[0m, in \u001b[0;36mcatch_xla_runtime_error.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: ANN202\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m xla_client\u001b[38;5;241m.\u001b[39mXlaRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;66;03m# === `max_steps` reached error\u001b[39;00m\n\u001b[0;32m     59\u001b[0m         eqx_max_steps_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEqxRuntimeError: The maximum number of method steps was reached. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     61\u001b[0m         )\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\jax\\_src\\pjit.py:341\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mno_tracing\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[0;32m    337\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info\u001b[38;5;241m.\u001b[39mfun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    338\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`jit`, but \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_tracing\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    340\u001b[0m (outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked, executable,\n\u001b[1;32m--> 341\u001b[0m  pgle_profiler) \u001b[38;5;241m=\u001b[39m \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m maybe_fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(\n\u001b[0;32m    344\u001b[0m     executable, out_tree, args_flat, out_flat, attrs_tracked, jaxpr\u001b[38;5;241m.\u001b[39meffects,\n\u001b[0;32m    345\u001b[0m     jaxpr\u001b[38;5;241m.\u001b[39mconsts, jit_info\u001b[38;5;241m.\u001b[39mabstracted_axes,\n\u001b[0;32m    346\u001b[0m     pgle_profiler)\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs, maybe_fastpath_data, _need_to_rebuild_with_fdo(pgle_profiler)\n",
      "File \u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\jax\\_src\\pjit.py:195\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[1;34m(fun, jit_info, *args, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m   args_flat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(core\u001b[38;5;241m.\u001b[39mfull_lower, args_flat)\n\u001b[0;32m    194\u001b[0m   core\u001b[38;5;241m.\u001b[39mcheck_eval_args(args_flat)\n\u001b[1;32m--> 195\u001b[0m   out_flat, compiled, profiler \u001b[38;5;241m=\u001b[39m \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m   out_flat \u001b[38;5;241m=\u001b[39m pjit_p\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs_flat, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mp\u001b[38;5;241m.\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\jax\\_src\\pjit.py:1657\u001b[0m, in \u001b[0;36m_pjit_call_impl_python\u001b[1;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, compiler_options_kvs, *args)\u001b[0m\n\u001b[0;32m   1645\u001b[0m compiler_options_kvs \u001b[38;5;241m=\u001b[39m compiler_options_kvs \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mtuple\u001b[39m(pgle_compile_options\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m   1646\u001b[0m \u001b[38;5;66;03m# Passing mutable PGLE profile here since it should be extracted by JAXPR to\u001b[39;00m\n\u001b[0;32m   1647\u001b[0m \u001b[38;5;66;03m# initialize the fdo_profile compile option.\u001b[39;00m\n\u001b[0;32m   1648\u001b[0m compiled \u001b[38;5;241m=\u001b[39m \u001b[43m_resolve_and_lower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1653\u001b[0m \u001b[43m    \u001b[49m\u001b[43minline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowering_platforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoweringParameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 1657\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1659\u001b[0m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compiled\u001b[38;5;241m.\u001b[39m_auto_spmd_lowering \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue:\n",
      "File \u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\jax\\_src\\interpreters\\pxla.py:2446\u001b[0m, in \u001b[0;36mMeshComputation.compile\u001b[1;34m(self, compiler_options)\u001b[0m\n\u001b[0;32m   2444\u001b[0m compiler_options_kvs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiler_options_kvs \u001b[38;5;241m+\u001b[39m t_compiler_options\n\u001b[0;32m   2445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options_kvs:\n\u001b[1;32m-> 2446\u001b[0m   executable \u001b[38;5;241m=\u001b[39m \u001b[43mUnloadedMeshExecutable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_hlo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2447\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2448\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2449\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compiler_options_kvs:\n\u001b[0;32m   2450\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;241m=\u001b[39m executable\n",
      "File \u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\jax\\_src\\interpreters\\pxla.py:2959\u001b[0m, in \u001b[0;36mUnloadedMeshExecutable.from_hlo\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   2956\u001b[0m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   2958\u001b[0m util\u001b[38;5;241m.\u001b[39mtest_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpxla_cached_compilation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2959\u001b[0m xla_executable \u001b[38;5;241m=\u001b[39m \u001b[43m_cached_compilation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspmd_lowering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtuple_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_spmd_lowering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_prop_to_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_prop_to_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpmap_nreps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2965\u001b[0m orig_out_shardings \u001b[38;5;241m=\u001b[39m out_shardings\n\u001b[0;32m   2967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_spmd_lowering:\n",
      "File \u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\jax\\_src\\interpreters\\pxla.py:2757\u001b[0m, in \u001b[0;36m_cached_compilation\u001b[1;34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_kvs, pgle_profiler)\u001b[0m\n\u001b[0;32m   2749\u001b[0m compile_options \u001b[38;5;241m=\u001b[39m create_compile_options(\n\u001b[0;32m   2750\u001b[0m     computation, mesh, spmd_lowering, tuple_args, auto_spmd_lowering,\n\u001b[0;32m   2751\u001b[0m     allow_prop_to_inputs, allow_prop_to_outputs, backend,\n\u001b[0;32m   2752\u001b[0m     dev, pmap_nreps, compiler_options)\n\u001b[0;32m   2754\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mlog_elapsed_time(\n\u001b[0;32m   2755\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{elapsed_time:.9f}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2756\u001b[0m     fun_name\u001b[38;5;241m=\u001b[39mname, event\u001b[38;5;241m=\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mBACKEND_COMPILE_EVENT):\n\u001b[1;32m-> 2757\u001b[0m   xla_executable \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_or_get_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2759\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2760\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable\n",
      "File \u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\jax\\_src\\compiler.py:473\u001b[0m, in \u001b[0;36mcompile_or_get_cached\u001b[1;34m(backend, computation, devices, compile_options, host_callbacks, pgle_profiler)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    472\u001b[0m   log_persistent_cache_miss(module_name, cache_key)\n\u001b[1;32m--> 473\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_and_write_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\jax\\_src\\compiler.py:690\u001b[0m, in \u001b[0;36m_compile_and_write_cache\u001b[1;34m(backend, computation, compile_options, host_callbacks, module_name, cache_key)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_compile_and_write_cache\u001b[39m(\n\u001b[0;32m    682\u001b[0m     backend: xc\u001b[38;5;241m.\u001b[39mClient,\n\u001b[0;32m    683\u001b[0m     computation: ir\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    687\u001b[0m     cache_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    688\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xc\u001b[38;5;241m.\u001b[39mLoadedExecutable:\n\u001b[0;32m    689\u001b[0m   start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 690\u001b[0m   executable \u001b[38;5;241m=\u001b[39m \u001b[43mbackend_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    693\u001b[0m   compile_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m    694\u001b[0m   _cache_write(\n\u001b[0;32m    695\u001b[0m       cache_key, compile_time, module_name, backend, executable, host_callbacks\n\u001b[0;32m    696\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\jax\\_src\\profiler.py:334\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    333\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[1;32mc:\\Users\\Fatem\\miniconda3\\envs\\optctrl\\Lib\\site-packages\\jax\\_src\\compiler.py:318\u001b[0m, in \u001b[0;36mbackend_compile\u001b[1;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    315\u001b[0m   \u001b[38;5;66;03m# we use a separate function call to ensure that XLA compilation appears\u001b[39;00m\n\u001b[0;32m    316\u001b[0m   \u001b[38;5;66;03m# separately in Python profiling results\u001b[39;00m\n\u001b[0;32m    317\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m host_callbacks:\n\u001b[1;32m--> 318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost_callbacks\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m   \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[0;32m    322\u001b[0m   \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[0;32m    323\u001b[0m   \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m    324\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compare the evolution before and after SPSA: \n",
    "my_test_params = my_params.copy()\n",
    "det_qb = 0\n",
    "det_osc = 0\n",
    "\n",
    "e_ops_evo = []\n",
    "e_ops_evo.extend([(dq.tensor(dq.proj(dq.basis(ncav, i)), dq.proj(dq.basis(ntr,0)) + dq.proj(dq.basis(ntr,1))  )) for i in range(ncav)])\n",
    "e_ops_qb = []\n",
    "e_ops_qb.extend([dq.tensor(idcav, dq.proj(dq.basis(ntr, i))) for i in range(ntr)])\n",
    "\n",
    "my_params['e_ops_qb'] = e_ops_qb\n",
    "my_params['e_ops_evo'] = e_ops_evo\n",
    "\n",
    "T1q = 5000\n",
    "T2q = 5000\n",
    "Tphiq = (1/T2q - 1/(2*T1q))**(-1)\n",
    "\n",
    "T1c = 20000\n",
    "T2c = 20000\n",
    "Tphic = (1/T2c - 1/(2*T1c))**(-1)\n",
    "\n",
    "c_ops = [\n",
    "        jnp.sqrt(1/T1q)*dq.tensor(idcav, t),\n",
    "        jnp.sqrt(1/Tphiq)*dq.tensor(idcav, tdag @ t),\n",
    "        jnp.sqrt(1/T1c)*dq.tensor(a, idtr), \n",
    "        jnp.sqrt(1/Tphic)*dq.tensor(adag @ a, idtr),\n",
    "        ]\n",
    "\n",
    "exp_state = [dq.tensor(dq.basis(ncav,2), dq.basis(ntr,0))]\n",
    "exp_code_dm_list = [psi @ psi.dag() for psi in exp_state]\n",
    "exp_code_dm = dq.stack(exp_code_dm_list)\n",
    "\n",
    "my_params['T1q'] = T1q\n",
    "my_params['T2q'] = T2q\n",
    "my_params['T1c'] = T1c\n",
    "my_params['T2c'] = T2c\n",
    "my_params['c_ops'] = c_ops\n",
    "my_params['exp_state'] = exp_state\n",
    "my_params['exp_code_dm'] = exp_code_dm\n",
    "\n",
    "def compute_evolution_mesolve(amps, params):\n",
    "    \"\"\"\n",
    "    Compute the evolution of the system given the pulse amplitudes and parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    a, adag = dq.destroy(params['ncav']), dq.create(params['ncav'])\n",
    "    t, tdag = dq.destroy(params['ntr']), dq.create(params['ntr'])\n",
    "    idcav = dq.eye(params['ncav'])\n",
    "    idtr = dq.eye(params['ntr'])\n",
    "    \n",
    "    if params['osc_drive'] == 'linear':\n",
    "        osc_pow = 1\n",
    "    elif params['osc_drive']=='squeeze':\n",
    "        osc_pow = 2\n",
    "\n",
    "    # Add noise to the drive\n",
    "    # iq_gain_errors = 1.0 + np.random.uniform(-0.001, 0.001, size=(4,))\n",
    "    # amps_real_noisy = np.real(amps) * np.array([[iq_gain_errors[0]], [iq_gain_errors[2]]])\n",
    "    # amps_imag_noisy = np.imag(amps) * np.array([[iq_gain_errors[1]], [iq_gain_errors[3]]])\n",
    "\n",
    "    # noisy_amps = amps_real_noisy + 1j * amps_imag_noisy\n",
    "    noisy_amps = amps\n",
    "\n",
    "    # Add detuning to the drives\n",
    "    qb_det_phi = det_qb * params['tpulse'][:-1]\n",
    "    osc_det_phi = det_osc * params['tpulse'][:-1]\n",
    "    det_osc_drive = noisy_amps[0,:] * jnp.exp(-1j*osc_det_phi)\n",
    "    det_qb_drive = noisy_amps[1,:] * jnp.exp(-1j*qb_det_phi)\n",
    "\n",
    "    # time-dependent Hamiltonian\n",
    "    # (sum of  piece-wise constant Hamiltonians and of the static Hamiltonian)\n",
    "    Hcr = dq.pwc(params['tpulse'], jnp.real(jnp.real(det_osc_drive)), dq.tensor(dq.powm(a,osc_pow) + dq.powm(adag,osc_pow), idtr))\n",
    "    Hci = dq.pwc(params['tpulse'], jnp.real(jnp.imag(det_osc_drive)), -1j * dq.tensor((dq.powm(a,osc_pow) - dq.powm(adag,osc_pow)), idtr))\n",
    "    Htr = dq.pwc(params['tpulse'], jnp.real(jnp.real(det_qb_drive)), dq.tensor(idcav, t+tdag))\n",
    "    Hti = dq.pwc(params['tpulse'], jnp.real(jnp.imag(det_qb_drive)), -1j * dq.tensor(idcav, (t - tdag)))\n",
    "    H = params['H0'] + Hcr + Hci + Htr + Hti\n",
    "\n",
    "    options = dq.Options(progress_meter = None)\n",
    "    solver = dq.method.Tsit5(max_steps = int(1e9))\n",
    "\n",
    "    evo_result = dq.mesolve(H, rho0=psi0, tsave=tsave, jump_ops = c_ops, exp_ops = exp_ops, options = options, method = solver)\n",
    "    \n",
    "    prob_sim = np.zeros((params['ncav'], len(params['tsave'])))\n",
    "    prob_qb_sim = np.zeros((params['ntr'], len(params['tsave'])))\n",
    "    \n",
    "    for i in range(evo_result.states.shape[1]):\n",
    "        for j in range(ncav):\n",
    "            prob_sim[j, i] = np.real(dq.ptrace(evo_result.states[0, i], dims = (ncav, ntr), keep = (0,)).to_jax()[j,j])\n",
    "\n",
    "    for i in range(evo_result.states.shape[1]):\n",
    "        for j in range(ntr):\n",
    "            prob_qb_sim[j,i] = np.real(dq.ptrace(evo_result.states[0, i], dims = (ncav, ntr), keep = (1,)).to_jax()[j,j])\n",
    "\n",
    "    return params['tsave'], prob_sim, prob_qb_sim\n",
    "\n",
    "tsave, prob_sim_gen, prob_qb_sim_gen = compute_evolution_mesolve(gen_pulse, my_test_params)\n",
    "_, prob_sim_spsa, prob_qb_sim_spsa = compute_evolution_mesolve(spsa_pulse, my_test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600edc3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prob_sim_spsa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m ax \u001b[38;5;241m=\u001b[39m axes[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(colors):\n\u001b[1;32m---> 25\u001b[0m     ax\u001b[38;5;241m.\u001b[39mplot(tsave, \u001b[43mprob_sim_spsa\u001b[49m[j, :], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$P_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39mc)\n\u001b[0;32m     26\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCavity Population (Post-SPSA)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m ax\u001b[38;5;241m.\u001b[39mlegend()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prob_sim_spsa' is not defined"
     ]
    }
   ],
   "source": [
    "colors = [\n",
    "    \"#0058A3\",\n",
    "    \"#008C6E\",\n",
    "    \"#D47A17\",\n",
    "    \"#B95D99\",\n",
    "    \"#6E6E6E\",\n",
    "    \"#7A4A91\",\n",
    "    \"#4A89C0\",\n",
    "    # \"#C04851\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10), sharex=True, sharey='row')\n",
    "\n",
    "# --- Top-Left: Cavity Population (Generated Pulse) ---\n",
    "ax = axes[0, 0]\n",
    "for j, c in enumerate(colors):\n",
    "    ax.plot(tsave, prob_sim_gen[j, :], '-', label=f'$P_{j}$', color=c)\n",
    "ax.set_title('Cavity Population (Pre-SPSA)')\n",
    "ax.set_ylabel('Population')\n",
    "ax.legend()\n",
    "\n",
    "# --- Top-Right: Cavity Population (SPSA Pulse) ---\n",
    "ax = axes[0, 1]\n",
    "for j, c in enumerate(colors):\n",
    "    ax.plot(tsave, prob_sim_spsa[j, :], '-', label=f'$P_{j}$', color=c)\n",
    "ax.set_title('Cavity Population (Post-SPSA)')\n",
    "ax.legend()\n",
    "\n",
    "# --- Bottom-Left: Qubit Population (Generated Pulse) ---\n",
    "ax = axes[1, 0]\n",
    "for j in range(ntr):\n",
    "    ax.plot(tsave, prob_qb_sim_gen[j, :], '-', label=f'$P_{j}$', color=colors[j])\n",
    "ax.set_title('Qubit Population (Pre-SPSA)')\n",
    "ax.set_ylabel('Population')\n",
    "ax.set_xlabel('Time ($\\\\mu s$)')\n",
    "ax.legend()\n",
    "\n",
    "# --- Bottom-Right: Qubit Population (SPSA Pulse) ---\n",
    "ax = axes[1, 1]\n",
    "for j in range(ntr):\n",
    "    ax.plot(tsave, prob_qb_sim_spsa[j, :], '-', label=f'$P_{j}$', color=colors[j])\n",
    "ax.set_title('Qubit Population (Post-SPSA)')\n",
    "ax.set_xlabel('Time ($\\\\mu s$)')\n",
    "ax.legend()\n",
    "\n",
    "# Apply grids to all subplots\n",
    "for ax_row in axes:\n",
    "    for ax in ax_row:\n",
    "        ax.grid(True, which='major', linestyle='-', linewidth='0.6', color='gray')\n",
    "        ax.grid(True, which='minor', linestyle=':', linewidth='0.5', color='lightgray')\n",
    "        ax.minorticks_on() \n",
    "        # ax.axhline(1, linestyle='--', color='k', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f131d7",
   "metadata": {},
   "source": [
    "### Robustness check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1347451b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Optimizer...\n",
      "Neural network model created.\n",
      "Model weights loaded successfully from C://Users//Fatem//SR_optctrl//Pulses//simulation_trained_bin_X_2025-09-02_13-20-53.weights.h5\n",
      "Initializing Optimizer...\n",
      "Neural network model created.\n",
      "Model weights loaded successfully from C://Users//Fatem//SR_optctrl//Pulses//simulation_trained_bin_X_robust_ordinary_2025-09-06_22-45-17.weights.h5\n"
     ]
    }
   ],
   "source": [
    "directory = 'C://Users//Fatem//SR_optctrl//Pulses//'\n",
    "\n",
    "optimizer = HardwareAwareOptimizer(my_params)\n",
    "optimizer.load_model_weights(directory + 'simulation_trained_bin_X_2025-09-02_13-20-53.weights.h5')\n",
    "\n",
    "optimizer2 = HardwareAwareOptimizer(my_params)\n",
    "optimizer2.load_model_weights(directory + 'simulation_trained_bin_X_robust_ordinary_2025-09-06_22-45-17.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a81559a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_calculator_func(amps, params):\n",
    "\n",
    "    a, adag = dq.destroy(params['ncav']), dq.create(params['ncav'])\n",
    "    t, tdag = dq.destroy(params['ntr']), dq.create(params['ntr'])\n",
    "    idcav = dq.eye(params['ncav'])\n",
    "    idtr = dq.eye(params['ntr'])\n",
    "    \n",
    "    if params['osc_drive'] == 'linear':\n",
    "        osc_pow = 1\n",
    "    elif params['osc_drive']=='squeeze':\n",
    "        osc_pow = 2\n",
    "\n",
    "    # time-dependent Hamiltonian\n",
    "    # (sum of  piece-wise constant Hamiltonians and of the static Hamiltonian)\n",
    "    Hcr = dq.pwc(params['tpulse'], jnp.real(jnp.real(amps[0,:])), dq.tensor(dq.powm(a,osc_pow) + dq.powm(adag,osc_pow), idtr))\n",
    "    Hci = dq.pwc(params['tpulse'], jnp.real(jnp.imag(amps[0,:])), -1j * dq.tensor((dq.powm(a,osc_pow) - dq.powm(adag,osc_pow)), idtr))\n",
    "    Htr = dq.pwc(params['tpulse'], jnp.real(jnp.real(amps[1,:])), dq.tensor(idcav, t+tdag))\n",
    "    Hti = dq.pwc(params['tpulse'], jnp.real(jnp.imag(amps[1,:])), -1j * dq.tensor(idcav, (t - tdag)))\n",
    "    H = params['H0'] + Hcr + Hci + Htr + Hti\n",
    "\n",
    "    options = dq.Options(progress_meter = None)\n",
    "    solver = dq.method.Tsit5(max_steps = int(1e9))\n",
    "\n",
    "    evo_result = dq.sesolve(H, psi0, tsave, exp_ops = exp_ops, options = options, method = solver)\n",
    "    \n",
    "    avg_gate_fidelity_loss = utl.compute_fidelity_loss(evo_result)\n",
    "\n",
    "    loss = 0\n",
    "    cnt=0\n",
    "\n",
    "    loss += 1*avg_gate_fidelity_loss\n",
    "    cnt+=1\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6149b546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating pulse from the trained model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(0.00515553, dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rob_pulse = optimizer.generate_pulse()\n",
    "loss_calculator_func(rob_pulse, my_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "751ba5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_robustness(amps, dets, params):\n",
    "\n",
    "    \"\"\"\n",
    "    Evaluate the robustness of the pulse to chi(dispersive coupling) detuning by computing the average gate fidelity\n",
    "    for different detuning values.\n",
    "\n",
    "    Parameters:\n",
    "    amps (ndarray): Optimized amplitude of shape (2, ntpulse-1) to be used to simulate the system.\n",
    "    dets (ndarray): Detuning values for the qubit frequency to be used to simulate the system.\n",
    "                    Ideally, this should have both negative and positive values.\n",
    "                    Each value of det will be used for a different simulation with an addition of\n",
    "                    det[i] * dq.tensor(a @ adag, t @ tdag) to params['H0'].\n",
    "    params (dict): The usual params dictionary with all necessary parameters for simulation.\n",
    "\n",
    "    Returns:\n",
    "    ndarray: Average gate fidelity for binomial gate operations for the different detuning values,\n",
    "             same shape as dets.\n",
    "    \"\"\"\n",
    "\n",
    "    fids = np.empty(len(dets))\n",
    "\n",
    "    a, adag = dq.destroy(params['ncav']), dq.create(params['ncav'])\n",
    "    t, tdag = dq.destroy(params['ntr']), dq.create(params['ntr'])\n",
    "    idcav = dq.eye(params['ncav'])\n",
    "    idtr = dq.eye(params['ntr'])\n",
    "    \n",
    "    if params['osc_drive'] == 'linear':\n",
    "        osc_pow = 1\n",
    "    elif params['osc_drive']=='squeeze':\n",
    "        osc_pow = 2\n",
    "\n",
    "    # time-dependent Hamiltonian\n",
    "    # (sum of  piece-wise constant Hamiltonians and of the static Hamiltonian)\n",
    "    Hcr = dq.pwc(params['tpulse'], jnp.real(jnp.real(amps[0,:])), dq.tensor(dq.powm(a,osc_pow) + dq.powm(adag,osc_pow), idtr))\n",
    "    Hci = dq.pwc(params['tpulse'], jnp.real(jnp.imag(amps[0,:])), -1j * dq.tensor((dq.powm(a,osc_pow) - dq.powm(adag,osc_pow)), idtr))\n",
    "    Htr = dq.pwc(params['tpulse'], jnp.real(jnp.real(amps[1,:])), dq.tensor(idcav, t+tdag))\n",
    "    Hti = dq.pwc(params['tpulse'], jnp.real(jnp.imag(amps[1,:])), -1j * dq.tensor(idcav, (t - tdag)))\n",
    "    \n",
    "    orig_H0 = params['H0'] + Hcr + Hci + Htr + Hti\n",
    "    options = dq.Options(progress_meter = None)\n",
    "    solver = dq.method.Tsit5(max_steps = int(1e9))\n",
    "\n",
    "    for i, det in enumerate(dets): \n",
    "        H = orig_H0 + det * dq.tensor(a @ adag, t @ tdag)\n",
    "        evo_result = dq.sesolve(H, psi0, tsave, exp_ops = exp_ops, options = options, method = solver)        \n",
    "        avg_gate_fidelity_loss = utl.compute_fidelity_loss(evo_result)\n",
    "        fids[i] = 1 - avg_gate_fidelity_loss\n",
    "\n",
    "    return fids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e7ae8939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99884081]\n",
      "0.9988408\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'robust_loss_calculator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(chi_robustness(rob_pulse, dets, my_params))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m loss_calculator_func(rob_pulse, my_params))\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[43mrobust_loss_calculator\u001b[49m(rob_pulse, my_params))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'robust_loss_calculator' is not defined"
     ]
    }
   ],
   "source": [
    "dets = [0]\n",
    "print(chi_robustness(rob_pulse, dets, my_params))\n",
    "print(1 - loss_calculator_func(rob_pulse, my_params))\n",
    "print(1-robust_loss_calculator(rob_pulse, my_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "315d2752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating pulse from the trained model...\n",
      "Generating pulse from the trained model...\n"
     ]
    }
   ],
   "source": [
    "### Check robustness of the fine-tuned pulse: \n",
    "chi_dets = np.linspace(-0.04, 0.04, 11)*(2*jnp.pi)\n",
    "\n",
    "rob_pulse = optimizer2.generate_pulse()\n",
    "rob_fid = chi_robustness(rob_pulse, chi_dets, my_params)\n",
    "\n",
    "gen_pulse = optimizer.generate_pulse()\n",
    "gen_fid = chi_robustness(gen_pulse, chi_dets, my_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4c0b8bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Fidelity')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(chi_dets/(2*jnp.pi), rob_fid, 'o-', label='Robust Pulse')\n",
    "plt.plot(chi_dets/(2*jnp.pi), gen_fid, 's--', label='Fidelity Pulse')\n",
    "plt.legend()\n",
    "plt.xlabel('Chi Detuning (MHz)')\n",
    "plt.ylabel('Fidelity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1442dee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73793972 0.81619304 0.8846668  0.93899035 0.97533107 0.99086517\n",
      " 0.98416179 0.95543277 0.90657848 0.84103799 0.76343006]\n",
      "[-0.075 -0.06  -0.045 -0.03  -0.015  0.     0.015  0.03   0.045  0.06\n",
      "  0.075]\n"
     ]
    }
   ],
   "source": [
    "print(gen_fid)\n",
    "print(chi_dets/(2*jnp.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90097e54",
   "metadata": {},
   "source": [
    "### ET check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "054f8fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating pulse from the trained model...\n",
      "0.009472182\n"
     ]
    }
   ],
   "source": [
    "amps = optimizer.generate_pulse()\n",
    "\n",
    "psi0 = [dq.tensor(L0, dq.basis(ntr, 0)), \n",
    "        dq.tensor(L1, dq.basis(ntr, 0)),\n",
    "        dq.tensor(plus_X, dq.basis(ntr, 0)),\n",
    "        dq.tensor(minus_X, dq.basis(ntr, 0)),\n",
    "        dq.tensor(plus_Y, dq.basis(ntr, 0)),\n",
    "        dq.tensor(minus_Y, dq.basis(ntr, 0))\n",
    "]\n",
    "\n",
    "exp_ops = [dq.tensor(dq.proj(L1), dq.proj(dq.basis(ntr, 0))), \n",
    "           dq.tensor(dq.proj(L0), dq.proj(dq.basis(ntr, 0))),\n",
    "           dq.tensor(dq.proj(plus_X), dq.proj(dq.basis(ntr, 0))), \n",
    "           dq.tensor(dq.proj(minus_X), dq.proj(dq.basis(ntr, 0))),\n",
    "           dq.tensor(dq.proj(minus_Y), dq.proj(dq.basis(ntr, 0))), \n",
    "           dq.tensor(dq.proj(plus_Y), dq.proj(dq.basis(ntr, 0)))\n",
    "]\n",
    "\n",
    "a, adag = dq.destroy(my_params['ncav']), dq.create(my_params['ncav'])\n",
    "t, tdag = dq.destroy(my_params['ntr']), dq.create(my_params['ntr'])\n",
    "idcav = dq.eye(my_params['ncav'])\n",
    "idtr = dq.eye(my_params['ntr'])\n",
    "\n",
    "if my_params['osc_drive'] == 'linear':\n",
    "        osc_pow = 1\n",
    "elif my_params['osc_drive']=='squeeze':\n",
    "        osc_pow = 2\n",
    "\n",
    "# time-dependent Hamiltonian\n",
    "# (sum of  piece-wise constant Hamiltonians and of the static Hamiltonian)\n",
    "Hcr = dq.pwc(my_params['tpulse'], jnp.real(jnp.real(amps[0,:])), dq.tensor(dq.powm(a,osc_pow) + dq.powm(adag,osc_pow), idtr))\n",
    "Hci = dq.pwc(my_params['tpulse'], jnp.real(jnp.imag(amps[0,:])), -1j * dq.tensor((dq.powm(a,osc_pow) - dq.powm(adag,osc_pow)), idtr))\n",
    "Htr = dq.pwc(my_params['tpulse'], jnp.real(jnp.real(amps[1,:])), dq.tensor(idcav, t+tdag))\n",
    "Hti = dq.pwc(my_params['tpulse'], jnp.real(jnp.imag(amps[1,:])), -1j * dq.tensor(idcav, (t - tdag)))\n",
    "H = my_params['H0'] + Hcr + Hci + Htr + Hti\n",
    "\n",
    "options = dq.Options(progress_meter = None)\n",
    "solver = dq.method.Tsit5(max_steps = int(1e9))\n",
    "\n",
    "evo_result = dq.sesolve(H, psi0, tsave, exp_ops = exp_ops, options = options, method = solver)\n",
    "\n",
    "avg_gate_fidelity_loss = utl.compute_fidelity_loss(evo_result)\n",
    "\n",
    "print(avg_gate_fidelity_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf238acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_even = sum(\n",
    "    dq.tensor(\n",
    "        dq.proj(dq.basis(my_params['ncav'], i)),\n",
    "        dq.proj(dq.basis(my_params['ntr'], 0))\n",
    "    )\n",
    "    for i in range(my_params['ncav'])\n",
    "    if i % 2 == 0\n",
    ")\n",
    "\n",
    "proj_odd = sum(\n",
    "    dq.tensor(\n",
    "        dq.proj(dq.basis(my_params['ncav'], i)),\n",
    "        dq.proj(dq.basis(my_params['ntr'], 0))\n",
    "    )\n",
    "    for i in range(my_params['ncav'])\n",
    "    if i % 2 == 1\n",
    ")\n",
    "\n",
    "T1q = 80\n",
    "\n",
    "T2q = 80\n",
    "Tphiq = (1/T2q - 1/(2*T1q))**(-1)\n",
    "\n",
    "T1c = 200\n",
    "T2c = 200\n",
    "Tphic = (1/T2c - 1/(2*T1c))**(-1)\n",
    "\n",
    "\n",
    "c_ops = [jnp.sqrt(1/T1q)*dq.tensor(idcav, t),\n",
    "        jnp.sqrt(1/Tphiq)*dq.tensor(idcav, tdag @ t),\n",
    "        jnp.sqrt(1/T1c)*dq.tensor(a, idtr), \n",
    "        jnp.sqrt(1/Tphic)*dq.tensor(adag @ a, idtr),\n",
    "        ]\n",
    "\n",
    "options = dq.Options(progress_meter = None)\n",
    "solver = dq.method.Tsit5(max_steps = int(1e9))\n",
    "\n",
    "evo_result = dq.mesolve(H, rho0 = psi0, tsave = my_params['tsave'], jump_ops = c_ops, method = solver, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9bdebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqrtm_jax(A, tol=1e-5):\n",
    "    \"\"\"\n",
    "    Compute the matrix square root using JAX.\n",
    "\n",
    "    Parameters:\n",
    "    A (jnp.ndarray or DenseQArray): Input matrix.\n",
    "    tol (float): Tolerance below which eigenvalues are considered zero.\n",
    "\n",
    "    Returns:\n",
    "    jnp.ndarray: Matrix square root of the input matrix.\n",
    "    \"\"\"\n",
    "    if isinstance(A, dq.qarrays.dense_qarray.DenseQArray):\n",
    "        A = jnp.array(A.to_jax())  # Convert DenseQArray to JAX array\n",
    "\n",
    "    # Ensure input matrix is valid\n",
    "    if jnp.any(jnp.isnan(A)) or jnp.any(jnp.isinf(A)):\n",
    "        raise ValueError(\"Input matrix contains NaNs or infinities\")\n",
    "\n",
    "    # Ensure input matrix is square\n",
    "    if A.ndim != 2 or A.shape[0] != A.shape[1]:\n",
    "        raise ValueError(\"Input must be a square matrix\")\n",
    "\n",
    "    # Ensure input matrix is Hermitian\n",
    "    if not jnp.allclose(A, A.conj().T):\n",
    "        raise ValueError(\"Input matrix must be Hermitian\")\n",
    "\n",
    "    # Compute the matrix square root\n",
    "    eigvals, eigvecs = jnp.linalg.eigh(A)\n",
    "\n",
    "    # Set small negative eigenvalues to zero\n",
    "    eigvals = jnp.where(eigvals < tol, 0, eigvals)\n",
    "\n",
    "    sqrt_eigvals = jnp.sqrt(eigvals)\n",
    "    sqrt_A = eigvecs @ jnp.diag(sqrt_eigvals) @ jnp.conj(eigvecs.T)\n",
    "\n",
    "    return sqrt_A\n",
    "\n",
    "def den_fidelity(rho, sigma):\n",
    "    \"\"\"Compute the fidelity between two density matrices.\"\"\"\n",
    "    \n",
    "    sqrt_rho = sqrtm_jax(rho)\n",
    "    product = sqrt_rho @ sigma @ sqrt_rho\n",
    "    sqrt_product = sqrtm_jax(product)\n",
    "\n",
    "    return jnp.real(jnp.trace(sqrt_product)) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c3388b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_state = [dq.tensor(L1, dq.basis(my_params['ntr'], 0)), \n",
    "                dq.tensor(L0, dq.basis(my_params['ntr'], 0)), \n",
    "                dq.tensor(plus_X, dq.basis(my_params['ntr'], 0)), \n",
    "                dq.tensor(minus_X, dq.basis(my_params['ntr'], 0)), \n",
    "                dq.tensor(minus_Y, dq.basis(my_params['ntr'], 0)), \n",
    "                dq.tensor(plus_Y, dq.basis(my_params['ntr'], 0))\n",
    "]\n",
    "target_den = [target_state[i] @ target_state[i].dag() for i in range(len(target_state))]\n",
    "recovery = dq.tensor( L1 @ E1.dag() + L0 @ E0.dag() , dq.eye(my_params['ntr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca962e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96793246\n"
     ]
    }
   ],
   "source": [
    "print(sum([den_fidelity(evo_result.states[i,-1], target_den[i]) for i in range(len(target_state))])/len(target_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04d7b0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9921475\n"
     ]
    }
   ],
   "source": [
    "print(sum([den_fidelity(dq.unit(proj_even @ evo_result.states[i,-1] @ proj_even), target_den[i]) for i in range(len(target_state))])/ len(target_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb3dc9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8841645\n"
     ]
    }
   ],
   "source": [
    "print(sum([den_fidelity(dq.unit(recovery @ dq.unit(proj_odd @ evo_result.states[i,-1] @ proj_odd) @ recovery.dag()), target_den[i]) for i in range(len(target_state))])/ len(target_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc76b97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optctrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
